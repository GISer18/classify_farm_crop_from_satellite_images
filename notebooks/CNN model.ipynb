{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Input, Activation, Conv2D, MaxPooling2D\n",
    "from keras import optimizers, callbacks \n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization,Add,Dropout, Activation, Flatten\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils, plot_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.  977.  998. 1069.    0.    0.    0.\n",
      "     0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0. 1062.  913.  875.  935.  966.  950. 1018.\n",
      "  1073.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0. 1040.  887.  889.  947.  914.  905.  902.\n",
      "     0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.  952.  873.  893.  933.  900.  904.  913.\n",
      "     0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.  933.  892.  897.  894.  883.  876.    0.\n",
      "     0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.  918.  878.  870.  863.  862.  899.    0.\n",
      "     0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0. 1028.  896.  894.  874.  854.  880.    0.    0.\n",
      "     0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.  991.  881.  866.  851.  838.  913.    0.    0.\n",
      "     0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.  926.  865.  852.  846.  862.    0.    0.    0.\n",
      "     0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.  915.  857.  842.  832.  935.    0.    0.    0.\n",
      "     0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.  866.  840.  835.  872.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.  872.  876.  881.  965.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.]]\n",
      "(5, 16, 16)\n",
      "[[   0.          0.          0.          0.          0.          0.\n",
      "   256.62662   262.64703   292.14453     0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.        130.3394\n",
      "   919.9827    939.21594  1035.7134    116.67596   119.054955  123.07699\n",
      "   130.51437     0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.       1004.50397\n",
      "   875.02875   877.4498    906.7535    899.2021    917.5366    948.53375\n",
      "  1005.8525      0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.        998.299\n",
      "   866.97437   866.52124   919.82336   910.30194   890.8441    893.8809\n",
      "   178.9787      0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.        951.423\n",
      "   858.57904   881.7926    915.8591    883.1101    873.404     888.5203\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.        919.3088\n",
      "   859.39636   889.8092    896.0558    868.0403    870.41425   423.82687\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.        900.81976\n",
      "   866.49976   883.35095   872.5577    863.46857   876.9576      0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        222.47969   885.85297\n",
      "   873.0184    871.02655   857.45984   861.90857   687.2479      0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        985.4144    874.027\n",
      "   875.4726    863.6538    856.0184    869.21576     0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        964.0331    868.1968\n",
      "   866.7693    855.4268    853.73975   897.887       0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        932.99817   862.1651\n",
      "   860.3134    848.53357   856.6541    202.90141     0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        901.9424    856.4891\n",
      "   847.7947    845.85236   900.8339      0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        876.7057    850.18933\n",
      "   843.9055    854.4623    487.33838     0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        875.7923    851.82935\n",
      "   853.5913    889.2173      0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        744.33167   714.012\n",
      "   716.92053   777.155       0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]]\n",
      "(5, 16, 16)\n",
      "[[   0.         0.         0.         0.         0.      1176.4624\n",
      "  1258.219   1288.7755  1287.9531  1181.8464  1206.3027  1228.7848\n",
      "  1227.3918     0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.      1294.7277\n",
      "  1255.7388  1270.4415  1284.9022  1305.3131  1311.7721  1333.733\n",
      "   585.8127     0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.      1292.789\n",
      "  1256.6995  1263.7126  1293.6283  1306.8765  1303.0278  1323.3878\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.      1287.4467\n",
      "  1263.4097  1281.343   1302.6643  1301.3424  1313.2119  1335.1292\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.      1303.1399\n",
      "  1277.7905  1292.5667  1305.493   1306.9418  1322.6906   615.3695\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.      1319.0294\n",
      "  1295.252   1303.5026  1308.6471  1313.9243  1327.6478     0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.      1324.6698\n",
      "  1315.9824  1317.0297  1315.7181  1318.4608  1322.4365     0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.       671.84265 1318.2754\n",
      "  1321.4689  1323.107   1324.6917  1322.5793   643.108      0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.      1309.4059  1312.344\n",
      "  1324.5905  1328.2225  1332.9751  1325.4592     0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.      1311.2059  1317.3706\n",
      "  1333.1974  1336.5695  1337.7716  1325.1495     0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.      1315.1179  1319.7482\n",
      "  1339.3308  1335.6844  1331.5984   680.1224     0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.      1318.3859  1319.9657\n",
      "  1341.159   1333.1359  1326.3685     0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.      1316.7305  1315.8198\n",
      "  1330.7875  1336.2032  1338.181      0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.      1316.1754  1319.8568\n",
      "  1333.6646  1337.916    724.2921     0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.      1321.7361  1326.2386\n",
      "  1341.6472  1341.1099     0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.      1353.1124  1327.2703\n",
      "  1346.9552  1353.7407     0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]]\n",
      "(5, 16, 16)\n",
      "[[   0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.         23.079277\n",
      "   959.7931    984.70715  1075.851      20.527401   20.369127   21.851652\n",
      "    23.371607    0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.       1057.4413\n",
      "   886.72253   858.51135   917.4671    936.18854   927.58875   981.97754\n",
      "   862.12854     0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.        999.3139\n",
      "   861.0447    871.8181    922.2818    889.3282    881.2908    885.86646\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.        930.0098\n",
      "   860.037     874.6451    889.39294   867.31995   865.00745   397.1529\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.        903.2655\n",
      "   858.34015   855.7363    847.51807   844.8814    873.2513      0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        932.65643   875.68427\n",
      "   867.21643   847.5458    830.3235    856.719      78.28106     0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        978.13727   855.862\n",
      "   842.1289    825.1516    815.90607   815.8397      0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        912.4231    839.1694\n",
      "   824.9552    817.0428    863.64435     0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        875.14166   825.5323\n",
      "   815.469     827.67096   514.7456      0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        854.3266    839.6524\n",
      "   840.37317   915.51263     0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        171.32396   170.29898\n",
      "   171.01799   190.4204      0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]]\n",
      "(5, 16, 16)\n",
      "[[   0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.         0.\n",
      "   366.9605   377.6027   410.62927    0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.       336.06592\n",
      "  1105.0509  1134.8813  1206.7833   323.27353  331.47076  337.83954\n",
      "   344.96024    0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.      1171.0776\n",
      "  1086.0547  1104.3134  1116.0602  1131.9375  1148.8691  1168.1807\n",
      "   914.7017     0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.      1152.1099\n",
      "  1084.467   1090.9308  1129.8821  1130.9257  1116.4003  1129.4088\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.      1132.1515\n",
      "  1088.4393  1115.7424  1131.0636  1112.5087  1120.6293   967.69904\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.      1137.1764\n",
      "  1104.4514  1122.8961  1121.6461  1118.1537  1131.6324     0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.        76.83521 1132.9802\n",
      "  1127.0177  1128.2218  1119.6437  1122.3223  1058.2166     0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.      1164.427   1120.2998\n",
      "  1128.1105  1126.3389  1127.8557  1126.6802     0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.      1157.2925  1122.823\n",
      "  1133.6012  1131.2034  1132.6528  1140.3901     0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.      1149.66    1122.4469\n",
      "  1135.9835  1124.3586  1123.4377    75.25933    0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.      1130.6456  1116.9225\n",
      "  1120.4908  1127.3816  1157.8997     0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.      1122.4846  1119.9652\n",
      "  1128.4767  1133.0078   178.92001    0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.      1159.7764  1125.4952\n",
      "  1138.1084  1166.8604     0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.       282.8544   272.56104\n",
      "   275.78983  284.24255    0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]]\n",
      "(5, 16, 16)\n",
      "[[   0.          0.          0.          0.          0.        494.43045\n",
      "  1161.1755   1192.5619   1247.9026    483.87387   495.23544   504.6362\n",
      "   510.7766      0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.       1218.1417\n",
      "  1149.3939   1172.3667   1174.9523   1194.264    1216.9077   1238.9268\n",
      "  1105.636       0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.       1209.8365\n",
      "  1147.4067   1153.1273   1186.2281   1198.6158   1189.7466   1204.9207\n",
      "   202.2904      0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.       1197.1412\n",
      "  1149.742    1164.6675   1192.8167   1189.3907   1187.8323   1207.2603\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.       1192.7256\n",
      "  1157.1202   1180.1289   1194.4263   1183.5464   1194.7955    882.8927\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.       1203.7946\n",
      "  1170.6559   1187.545    1190.1538   1189.5402   1204.342      11.585809\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.       1204.5282\n",
      "  1190.3479   1195.7555   1191.6134   1193.3682   1203.0682      0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        516.97186  1198.7881\n",
      "  1198.977    1199.4856   1196.8284   1197.03      692.05        0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.       1217.356    1191.3115\n",
      "  1200.9343   1201.1125   1203.8776   1201.1489      0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.       1214.7047   1194.5281\n",
      "  1206.9955   1206.7727   1208.0773   1207.5109      0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.       1212.7819   1195.4612\n",
      "  1210.7686   1203.8518   1202.042     513.10364     0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.       1207.6448   1193.8616\n",
      "  1207.7216   1202.1056   1206.2001      0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.       1197.6753   1189.5277\n",
      "  1195.8715   1205.1287   1216.7882      0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.       1194.5002   1194.5557\n",
      "  1204.622    1208.3595    335.9411      0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.       1214.482    1198.4231\n",
      "  1212.011    1224.216       0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.       1041.7987   1008.0535\n",
      "  1021.6732   1043.0522      0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]]\n",
      "(5, 16, 16)\n",
      "[[  0.        0.        0.        0.        0.        0.      798.19617\n",
      "  817.4635  920.86194   0.        0.        0.        0.        0.\n",
      "    0.        0.     ]\n",
      " [  0.        0.        0.        0.        0.      667.67737 878.1397\n",
      "  895.41876 947.13434 594.44946 614.1045  627.2722  666.5529    0.\n",
      "    0.        0.     ]\n",
      " [  0.        0.        0.        0.        0.      999.90576 860.06476\n",
      "  867.60114 902.8467  906.2686  908.64307 918.6791  574.0982    0.\n",
      "    0.        0.     ]\n",
      " [  0.        0.        0.        0.        0.      965.5335  856.68475\n",
      "  863.6607  913.5411  896.6777  869.7858  873.5496    0.        0.\n",
      "    0.        0.     ]\n",
      " [  0.        0.        0.        0.        0.      925.9222  849.58624\n",
      "  888.8784  905.6804  862.1437  859.09937 887.4557    0.        0.\n",
      "    0.        0.     ]\n",
      " [  0.        0.        0.        0.        0.      905.62067 852.87286\n",
      "  887.6442  883.79156 860.05664 866.8487  201.70628   0.        0.\n",
      "    0.        0.     ]\n",
      " [  0.        0.        0.        0.        0.      888.7684  862.8642\n",
      "  879.87445 865.6611  859.81244 876.99036   0.        0.        0.\n",
      "    0.        0.     ]\n",
      " [  0.        0.        0.        0.      322.63422 875.80994 870.2922\n",
      "  869.7305  856.41223 860.8227  590.8069    0.        0.        0.\n",
      "    0.        0.     ]\n",
      " [  0.        0.        0.        0.      971.85126 867.1689  870.02747\n",
      "  860.69    857.1112  866.14636   0.        0.        0.        0.\n",
      "    0.        0.     ]\n",
      " [  0.        0.        0.        0.      955.2138  863.8637  866.49976\n",
      "  856.4858  858.3615  892.62256   0.        0.        0.        0.\n",
      "    0.        0.     ]\n",
      " [  0.        0.        0.        0.      933.58606 860.5438  862.5236\n",
      "  849.9295  856.9289  296.8572    0.        0.        0.        0.\n",
      "    0.        0.     ]\n",
      " [  0.        0.        0.        0.      904.2168  856.4239  851.9399\n",
      "  848.15485 886.64777   0.        0.        0.        0.        0.\n",
      "    0.        0.     ]\n",
      " [  0.        0.        0.        0.      875.99005 851.89166 843.20966\n",
      "  852.47144 714.2018    0.        0.        0.        0.        0.\n",
      "    0.        0.     ]\n",
      " [  0.        0.        0.        0.      863.3706  847.90533 850.731\n",
      "  862.676     0.        0.        0.        0.        0.        0.\n",
      "    0.        0.     ]\n",
      " [  0.        0.        0.        0.      900.054   857.66675 860.66364\n",
      "  917.61487   0.        0.        0.        0.        0.        0.\n",
      "    0.        0.     ]\n",
      " [  0.        0.        0.        0.      395.9989  374.33478 375.65918\n",
      "  405.8241    0.        0.        0.        0.        0.        0.\n",
      "    0.        0.     ]]\n",
      "(5, 16, 16)\n",
      "[[   0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.       326.057\n",
      "   841.24854  872.6559  1035.4825   253.13101  265.89725  287.21863\n",
      "   334.4063     0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.      1017.2971\n",
      "   761.34753  760.54553  816.609    810.0832   795.4214   822.5923\n",
      "   351.13428    0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.       896.7821\n",
      "   722.42804  762.20374  789.1449   745.6144   734.081    735.45306\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.       813.4091\n",
      "   727.9121   742.4558   725.4831   720.5708   749.34985    0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.       811.02814  753.18665\n",
      "   723.6644   702.71985  692.83746  731.59875  145.80174    0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.       930.85693  720.2736\n",
      "   711.40955  680.526    683.1727   646.2216     0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.       829.6414   699.82776\n",
      "   683.3007   674.32544  810.972      0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.       745.9555   687.75635\n",
      "   688.29266  733.7461    50.89359    0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.       532.2704   493.55344\n",
      "   490.61972  607.7776     0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]]\n",
      "(5, 16, 16)\n",
      "[[   0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.          0.\n",
      "   393.42194   402.7501    450.09055     0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.        379.50555\n",
      "   901.54486   917.79047   996.2591    339.04034   347.54083   357.68497\n",
      "   379.58923     0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.       1012.6166\n",
      "   867.1605    871.3856    905.9803    912.91437   919.36053   937.2271\n",
      "   703.74036     0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.        971.64154\n",
      "   860.4385    868.83856   917.7397    897.1928    874.63165   878.95844\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.        924.6772\n",
      "   854.4194    889.49097   903.83496   866.76685   866.7384    713.6765\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.        902.09485\n",
      "   861.3434    886.51904   876.67566   862.71533   872.41235     0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.         83.96401   884.506\n",
      "   871.604     872.4528    857.1377    860.74756   810.0384      0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        981.7412    871.79803\n",
      "   873.8785    862.9957    856.4555    866.04584     0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        959.35425   866.2497\n",
      "   866.231     855.3741    855.43506   897.94714     0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        927.277     860.54486\n",
      "   860.018     847.5394    856.8887     76.826965    0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        890.74493   854.5884\n",
      "   843.7762    846.9052    916.0753      0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        867.8313    848.03705\n",
      "   846.8466    860.3631    184.64188     0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        893.8588    858.6092\n",
      "   861.51263   918.69275     0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.        283.39462   270.36362\n",
      "   271.41107   293.83902     0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]\n",
      " [   0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.          0.          0.\n",
      "     0.          0.          0.          0.      ]]\n",
      "(5, 16, 16)\n",
      "[[   0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.         0.\n",
      "   558.0829   572.04285  620.3081     0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.       531.4234\n",
      "   933.70544  927.3972   999.00525  476.8665   471.51224  505.60593\n",
      "   537.6737     0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.      1051.38\n",
      "   889.4642   869.39496  928.22986  932.5733   921.3435   959.80865\n",
      "   620.52716    0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.      1005.01074\n",
      "   869.6551   879.05304  930.3986   897.4841   890.5418   893.4685\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.       941.09283\n",
      "   864.088    881.89136  909.40656  881.37683  881.5816   658.3931\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.       920.6734\n",
      "   874.7387   878.79895  874.13556  864.83734  867.51605    0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.       118.29916  902.9674\n",
      "   865.6308   857.04517  847.24414  850.487    785.17145    0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.      1023.90186  881.74304\n",
      "   877.4521   857.01135  838.49384  868.23706    0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.       988.2405   867.17334\n",
      "   853.354    836.44354  823.7245   900.9898     0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.       928.6761   852.6434\n",
      "   838.98303  830.93243  846.01575  104.10479    0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.       907.4691   843.68005\n",
      "   829.00433  820.07245  914.50775    0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.       868.08026  830.3663\n",
      "   823.12146  848.3911   250.38069    0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.       860.65436  849.71423\n",
      "   850.9014   924.2487     0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.       365.3958   364.73953\n",
      "   366.49548  405.42776    0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]\n",
      " [   0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.         0.         0.\n",
      "     0.         0.         0.         0.     ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# https://www.pyimagesearch.com/2017/12/11/image-classification-with-keras-and-deep-learning/\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "img = np.load('croped_images/2017-01-01/train16/class8/index0.npy')  # this is a npy array\n",
    "img = np.transpose(img, (2,1,0))\n",
    "# print(img.shape)\n",
    "\n",
    "\n",
    "## From here https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/\n",
    "\n",
    "samples = np.expand_dims(img, 0)\n",
    "# print(samples.shape)\n",
    "\n",
    "# datagen = ImageDataGenerator(rotation_range=180, width_shift_range=0.05,\n",
    "#     height_shift_range=0.05, shear_range=0.2, zoom_range=0.6,\n",
    "#     horizontal_flip=True, fill_mode=\"nearest\")\n",
    "datagen = ImageDataGenerator(zoom_range=0.6)\n",
    "# prepare iterator\n",
    "it = datagen.flow(samples, batch_size=1)\n",
    "plt.imshow(img[1])\n",
    "plt.show()\n",
    "print(img[1])\n",
    "# generate samples and plot\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    # generate batch of images\n",
    "    batch = it.next()\n",
    "    # convert to unsigned integers for viewing\n",
    "    image = batch[0]\n",
    "    # plot raw pixel data\n",
    "    print(image.shape)\n",
    "    plt.imshow(image[1])\n",
    "    print(image[1])\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9514866979655712, 39.58730158730159, 2.9169590643274854, 0.5169983416252073, 0.9791912053396152, 1.8980213089802132, 1.0378693300041615, 0.29448577163773765, 3.5988455988455987]\n",
      "(2494, 16, 16, 5)\n",
      "(1074, 16, 16, 5)\n",
      "(2494, 9)\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "## Prepare the data\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "\n",
    "parent_path = 'croped_images/2017-01-01/train16/'\n",
    "for folder in os.listdir(parent_path):\n",
    "    if os.path.isdir(os.path.join(parent_path, folder)):\n",
    "        class_name = folder[-1]\n",
    "        for file in (os.listdir(os.path.join(parent_path, folder))):\n",
    "            x = np.load(os.path.join(parent_path, folder, file))\n",
    "            X_train.append(x)\n",
    "            y_train.append(class_name)\n",
    "            \n",
    "parent_path = 'croped_images/2017-01-01/test16/'\n",
    "for file in os.listdir(parent_path):\n",
    "    x = np.load(os.path.join(parent_path, file))\n",
    "    X_test.append(x)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)\n",
    "class_weights = list(class_weights)\n",
    "print(class_weights)\n",
    "            \n",
    "X_test = np.array(X_test)\n",
    "X_train = np.array(X_train)\n",
    "y_train_df = pd.DataFrame(y_train)\n",
    "y_train = np.array(pd.get_dummies(y_train_df))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "\n",
    "# https://stats.stackexchange.com/questions/175504/how-to-do-data-augmentation-and-train-validate-split\n",
    "# Data augmentation must be done only on training data and not the validation data since we want to simulate the \n",
    "# performance of the model on real world data\n",
    "datagen = ImageDataGenerator(rotation_range=180, width_shift_range=0.05,\n",
    "    height_shift_range=0.05, shear_range=0.2, zoom_range=0.6,\n",
    "    horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(inp_shape):\n",
    "    \n",
    "    #inp_shape = (16, 16, 5)\n",
    "    print(inp_shape)\n",
    "    inp = Input(shape= inp_shape)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3))(inp)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3))(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3))(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # the model so far outputs 3D feature maps (height, width, features)\n",
    "    \n",
    "    # this converts our 3D feature maps to 1D feature vectors\n",
    "#     print(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(16)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0)(x)\n",
    "    x = Dropout(0.5)(x) \n",
    "    \n",
    "    out = Dense(9, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=[out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16, 5)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        (None, 16, 16, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 14, 14, 32)        1472      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 12, 12, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)   (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)   (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                102416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 9)                 153       \n",
      "=================================================================\n",
      "Total params: 131,849\n",
      "Trainable params: 131,817\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_cnn_model(X_train[1].shape)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model():\n",
    "    model = create_cnn_model(X_train[1].shape)\n",
    "    adam = optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, label):\n",
    "    plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(history.history[\"acc\"], label=\"train_acc\")\n",
    "    plt.plot(history.history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.title('Training Loss and Accuracy  for %s' % label)\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/keras_preprocessing/image/numpy_array_iterator.py:127: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (2494, 16, 16, 5) (5 channels).\n",
      "  str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "39/38 [==============================] - 4s 90ms/step - loss: 2.3761 - acc: 0.2074\n",
      "Epoch 2/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 2.0108 - acc: 0.3709\n",
      "Epoch 3/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.8549 - acc: 0.4062\n",
      "Epoch 4/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.7676 - acc: 0.4226\n",
      "Epoch 5/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.7432 - acc: 0.4459\n",
      "Epoch 6/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.7003 - acc: 0.4711\n",
      "Epoch 7/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.6923 - acc: 0.4507\n",
      "Epoch 8/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.6803 - acc: 0.4567\n",
      "Epoch 9/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.6603 - acc: 0.4643\n",
      "Epoch 10/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.6344 - acc: 0.4707\n",
      "Epoch 11/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.6112 - acc: 0.4675\n",
      "Epoch 12/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.6462 - acc: 0.4636\n",
      "Epoch 13/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.6483 - acc: 0.4696\n",
      "Epoch 14/1000\n",
      "39/38 [==============================] - 2s 38ms/step - loss: 1.6498 - acc: 0.4515\n",
      "Epoch 15/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.6219 - acc: 0.4635\n",
      "Epoch 16/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.6231 - acc: 0.4691\n",
      "Epoch 17/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.6177 - acc: 0.4707\n",
      "Epoch 18/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.6109 - acc: 0.4720\n",
      "Epoch 19/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5916 - acc: 0.4820\n",
      "Epoch 20/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5824 - acc: 0.4836\n",
      "Epoch 21/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5885 - acc: 0.4824\n",
      "Epoch 22/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5890 - acc: 0.4804\n",
      "Epoch 23/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.6019 - acc: 0.4667\n",
      "Epoch 24/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.6044 - acc: 0.4655\n",
      "Epoch 25/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5842 - acc: 0.4768\n",
      "Epoch 26/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5734 - acc: 0.4875\n",
      "Epoch 27/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5774 - acc: 0.4864\n",
      "Epoch 28/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5835 - acc: 0.4879\n",
      "Epoch 29/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5630 - acc: 0.4843\n",
      "Epoch 30/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5903 - acc: 0.4837\n",
      "Epoch 31/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5775 - acc: 0.4728\n",
      "Epoch 32/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.5859 - acc: 0.4744\n",
      "Epoch 33/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5723 - acc: 0.4844\n",
      "Epoch 34/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5726 - acc: 0.4888\n",
      "Epoch 35/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5733 - acc: 0.4801\n",
      "Epoch 36/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5607 - acc: 0.4787\n",
      "Epoch 37/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5467 - acc: 0.4904\n",
      "Epoch 38/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5718 - acc: 0.4699\n",
      "Epoch 39/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.5629 - acc: 0.4876\n",
      "Epoch 40/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5699 - acc: 0.4812\n",
      "Epoch 41/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5534 - acc: 0.4772\n",
      "Epoch 42/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5633 - acc: 0.4775\n",
      "Epoch 43/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5516 - acc: 0.4884\n",
      "Epoch 44/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5538 - acc: 0.4980\n",
      "Epoch 45/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5453 - acc: 0.4839\n",
      "Epoch 46/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5445 - acc: 0.4879\n",
      "Epoch 47/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5463 - acc: 0.4919\n",
      "Epoch 48/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.5533 - acc: 0.4824\n",
      "Epoch 49/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.5660 - acc: 0.4840\n",
      "Epoch 50/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.5607 - acc: 0.4659\n",
      "Epoch 51/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.5469 - acc: 0.4859\n",
      "Epoch 52/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5247 - acc: 0.4872\n",
      "Epoch 53/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5500 - acc: 0.4879\n",
      "Epoch 54/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.5414 - acc: 0.4945\n",
      "Epoch 55/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.5579 - acc: 0.4864\n",
      "Epoch 56/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5470 - acc: 0.4876\n",
      "Epoch 57/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5438 - acc: 0.5016\n",
      "Epoch 58/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5621 - acc: 0.4807\n",
      "Epoch 59/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5422 - acc: 0.4940\n",
      "Epoch 60/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5365 - acc: 0.4904\n",
      "Epoch 61/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5430 - acc: 0.4929\n",
      "Epoch 62/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5333 - acc: 0.4984\n",
      "Epoch 63/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5420 - acc: 0.4920\n",
      "Epoch 64/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5308 - acc: 0.4932\n",
      "Epoch 65/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.5261 - acc: 0.4948\n",
      "Epoch 66/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5502 - acc: 0.4940\n",
      "Epoch 67/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5313 - acc: 0.4900\n",
      "Epoch 68/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5428 - acc: 0.4875\n",
      "Epoch 69/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5360 - acc: 0.4963\n",
      "Epoch 70/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5364 - acc: 0.4928\n",
      "Epoch 71/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5544 - acc: 0.4940\n",
      "Epoch 72/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5371 - acc: 0.4908\n",
      "Epoch 73/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5214 - acc: 0.5013\n",
      "Epoch 74/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5446 - acc: 0.4867\n",
      "Epoch 75/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5346 - acc: 0.4968\n",
      "Epoch 76/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5249 - acc: 0.5048\n",
      "Epoch 77/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5205 - acc: 0.5012\n",
      "Epoch 78/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.5218 - acc: 0.5044\n",
      "Epoch 79/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5212 - acc: 0.4936\n",
      "Epoch 80/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.5171 - acc: 0.4936\n",
      "Epoch 81/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5205 - acc: 0.4975\n",
      "Epoch 82/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4981 - acc: 0.5136\n",
      "Epoch 83/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5070 - acc: 0.5084\n",
      "Epoch 84/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5116 - acc: 0.5020\n",
      "Epoch 85/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5283 - acc: 0.4991\n",
      "Epoch 86/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5207 - acc: 0.5044\n",
      "Epoch 87/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5245 - acc: 0.4979\n",
      "Epoch 88/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5249 - acc: 0.5020\n",
      "Epoch 89/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5195 - acc: 0.5104\n",
      "Epoch 90/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5060 - acc: 0.5028\n",
      "Epoch 91/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5150 - acc: 0.4968\n",
      "Epoch 92/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5261 - acc: 0.4968\n",
      "Epoch 93/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5191 - acc: 0.5005\n",
      "Epoch 94/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5088 - acc: 0.5016\n",
      "Epoch 95/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5134 - acc: 0.4948\n",
      "Epoch 96/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5090 - acc: 0.4979\n",
      "Epoch 97/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5033 - acc: 0.4912\n",
      "Epoch 98/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5090 - acc: 0.5004\n",
      "Epoch 99/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5070 - acc: 0.5056\n",
      "Epoch 100/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5126 - acc: 0.5024\n",
      "Epoch 101/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.5117 - acc: 0.4972\n",
      "Epoch 102/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.5113 - acc: 0.5025\n",
      "Epoch 103/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5087 - acc: 0.5096\n",
      "Epoch 104/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.5023 - acc: 0.5072\n",
      "Epoch 105/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5135 - acc: 0.4940\n",
      "Epoch 106/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4976 - acc: 0.5007\n",
      "Epoch 107/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4951 - acc: 0.5096\n",
      "Epoch 108/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4882 - acc: 0.5152\n",
      "Epoch 109/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5066 - acc: 0.5074\n",
      "Epoch 110/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5132 - acc: 0.5032\n",
      "Epoch 111/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5060 - acc: 0.5081\n",
      "Epoch 112/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4996 - acc: 0.5055\n",
      "Epoch 113/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5111 - acc: 0.5021\n",
      "Epoch 114/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4987 - acc: 0.5072\n",
      "Epoch 115/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.5014 - acc: 0.5032\n",
      "Epoch 116/1000\n",
      "39/38 [==============================] - 2s 40ms/step - loss: 1.5069 - acc: 0.5004\n",
      "Epoch 117/1000\n",
      "39/38 [==============================] - 2s 42ms/step - loss: 1.4783 - acc: 0.5104\n",
      "Epoch 118/1000\n",
      "39/38 [==============================] - 2s 41ms/step - loss: 1.4903 - acc: 0.5008\n",
      "Epoch 119/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4966 - acc: 0.4968\n",
      "Epoch 120/1000\n",
      "39/38 [==============================] - 2s 42ms/step - loss: 1.4880 - acc: 0.5061\n",
      "Epoch 121/1000\n",
      "39/38 [==============================] - 2s 41ms/step - loss: 1.4880 - acc: 0.5192\n",
      "Epoch 122/1000\n",
      "39/38 [==============================] - 2s 40ms/step - loss: 1.5039 - acc: 0.5069\n",
      "Epoch 123/1000\n",
      "39/38 [==============================] - 2s 40ms/step - loss: 1.5064 - acc: 0.5048\n",
      "Epoch 124/1000\n",
      "39/38 [==============================] - 2s 40ms/step - loss: 1.4960 - acc: 0.5220\n",
      "Epoch 125/1000\n",
      "39/38 [==============================] - 2s 40ms/step - loss: 1.5030 - acc: 0.5044\n",
      "Epoch 126/1000\n",
      "39/38 [==============================] - 3s 64ms/step - loss: 1.4813 - acc: 0.5089\n",
      "Epoch 127/1000\n",
      "39/38 [==============================] - 2s 63ms/step - loss: 1.4964 - acc: 0.5096\n",
      "Epoch 128/1000\n",
      "39/38 [==============================] - 2s 61ms/step - loss: 1.4871 - acc: 0.5108\n",
      "Epoch 129/1000\n",
      "39/38 [==============================] - 2s 56ms/step - loss: 1.4922 - acc: 0.4994\n",
      "Epoch 130/1000\n",
      "39/38 [==============================] - 2s 40ms/step - loss: 1.4972 - acc: 0.4951\n",
      "Epoch 131/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4979 - acc: 0.5017\n",
      "Epoch 132/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4914 - acc: 0.5089\n",
      "Epoch 133/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4863 - acc: 0.5140\n",
      "Epoch 134/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4906 - acc: 0.5068\n",
      "Epoch 135/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4882 - acc: 0.5097\n",
      "Epoch 136/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4716 - acc: 0.5080\n",
      "Epoch 137/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4688 - acc: 0.5117\n",
      "Epoch 138/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4772 - acc: 0.5209\n",
      "Epoch 139/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4800 - acc: 0.5241\n",
      "Epoch 140/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4961 - acc: 0.5173\n",
      "Epoch 141/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4702 - acc: 0.5093\n",
      "Epoch 142/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4785 - acc: 0.5112\n",
      "Epoch 143/1000\n",
      "39/38 [==============================] - 2s 41ms/step - loss: 1.4782 - acc: 0.5073\n",
      "Epoch 144/1000\n",
      "39/38 [==============================] - 2s 40ms/step - loss: 1.4573 - acc: 0.5212\n",
      "Epoch 145/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4978 - acc: 0.5160\n",
      "Epoch 146/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4683 - acc: 0.5172\n",
      "Epoch 147/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4798 - acc: 0.5193\n",
      "Epoch 148/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4727 - acc: 0.5220\n",
      "Epoch 149/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4717 - acc: 0.5184\n",
      "Epoch 150/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4631 - acc: 0.5145\n",
      "Epoch 151/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4585 - acc: 0.5245\n",
      "Epoch 152/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4762 - acc: 0.5136\n",
      "Epoch 153/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4712 - acc: 0.5272\n",
      "Epoch 154/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4803 - acc: 0.5144\n",
      "Epoch 155/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4781 - acc: 0.5133\n",
      "Epoch 156/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4872 - acc: 0.5120\n",
      "Epoch 157/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4838 - acc: 0.5172\n",
      "Epoch 158/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4769 - acc: 0.5064\n",
      "Epoch 159/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4772 - acc: 0.5117\n",
      "Epoch 160/1000\n",
      "39/38 [==============================] - 2s 38ms/step - loss: 1.4520 - acc: 0.5223\n",
      "Epoch 161/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4516 - acc: 0.5188\n",
      "Epoch 162/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4855 - acc: 0.5124\n",
      "Epoch 163/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4771 - acc: 0.5105\n",
      "Epoch 164/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4537 - acc: 0.5250\n",
      "Epoch 165/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4529 - acc: 0.5305\n",
      "Epoch 166/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4709 - acc: 0.5212\n",
      "Epoch 167/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4863 - acc: 0.5044\n",
      "Epoch 168/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4498 - acc: 0.5136\n",
      "Epoch 169/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4630 - acc: 0.5241\n",
      "Epoch 170/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4657 - acc: 0.5276\n",
      "Epoch 171/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4531 - acc: 0.5244\n",
      "Epoch 172/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4716 - acc: 0.5240\n",
      "Epoch 173/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4667 - acc: 0.5232\n",
      "Epoch 174/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4409 - acc: 0.5305\n",
      "Epoch 175/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4516 - acc: 0.5177\n",
      "Epoch 176/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4682 - acc: 0.5241\n",
      "Epoch 177/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4473 - acc: 0.5213\n",
      "Epoch 178/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4438 - acc: 0.5336\n",
      "Epoch 179/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4426 - acc: 0.5265\n",
      "Epoch 180/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4549 - acc: 0.5272\n",
      "Epoch 181/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4704 - acc: 0.5116\n",
      "Epoch 182/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4469 - acc: 0.5212\n",
      "Epoch 183/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4503 - acc: 0.5365\n",
      "Epoch 184/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4578 - acc: 0.5141\n",
      "Epoch 185/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4634 - acc: 0.5217\n",
      "Epoch 186/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4462 - acc: 0.5249\n",
      "Epoch 187/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4630 - acc: 0.5124\n",
      "Epoch 188/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4473 - acc: 0.5260\n",
      "Epoch 189/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4674 - acc: 0.5253\n",
      "Epoch 190/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4540 - acc: 0.5213\n",
      "Epoch 191/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4739 - acc: 0.5165\n",
      "Epoch 192/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4682 - acc: 0.5248\n",
      "Epoch 193/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4625 - acc: 0.5152\n",
      "Epoch 194/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4691 - acc: 0.5220\n",
      "Epoch 195/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4624 - acc: 0.5300\n",
      "Epoch 196/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4694 - acc: 0.5160\n",
      "Epoch 197/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4509 - acc: 0.5217\n",
      "Epoch 198/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4493 - acc: 0.5193\n",
      "Epoch 199/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4536 - acc: 0.5280\n",
      "Epoch 200/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4721 - acc: 0.5125\n",
      "Epoch 201/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4515 - acc: 0.5272\n",
      "Epoch 202/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4397 - acc: 0.5285\n",
      "Epoch 203/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4530 - acc: 0.5233\n",
      "Epoch 204/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4403 - acc: 0.5313\n",
      "Epoch 205/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4435 - acc: 0.5316\n",
      "Epoch 206/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4645 - acc: 0.5269\n",
      "Epoch 207/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4533 - acc: 0.5289\n",
      "Epoch 208/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4460 - acc: 0.5233\n",
      "Epoch 209/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4701 - acc: 0.5132\n",
      "Epoch 210/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4419 - acc: 0.5296\n",
      "Epoch 211/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4445 - acc: 0.5284\n",
      "Epoch 212/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4626 - acc: 0.5193\n",
      "Epoch 213/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4470 - acc: 0.5273\n",
      "Epoch 214/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4586 - acc: 0.5261\n",
      "Epoch 215/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4431 - acc: 0.5380\n",
      "Epoch 216/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4493 - acc: 0.5221\n",
      "Epoch 217/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4369 - acc: 0.5256\n",
      "Epoch 218/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4609 - acc: 0.5206\n",
      "Epoch 219/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4409 - acc: 0.5330\n",
      "Epoch 220/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4555 - acc: 0.5104\n",
      "Epoch 221/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4661 - acc: 0.5157\n",
      "Epoch 222/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4628 - acc: 0.5116\n",
      "Epoch 223/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4306 - acc: 0.5292\n",
      "Epoch 224/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4441 - acc: 0.5297\n",
      "Epoch 225/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4428 - acc: 0.5268\n",
      "Epoch 226/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4492 - acc: 0.5216\n",
      "Epoch 227/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4575 - acc: 0.5196\n",
      "Epoch 228/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4370 - acc: 0.5236\n",
      "Epoch 229/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4408 - acc: 0.5265\n",
      "Epoch 230/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4517 - acc: 0.5332\n",
      "Epoch 231/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4258 - acc: 0.5265\n",
      "Epoch 232/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4286 - acc: 0.5326\n",
      "Epoch 233/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4314 - acc: 0.5289\n",
      "Epoch 234/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4251 - acc: 0.5284\n",
      "Epoch 235/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4463 - acc: 0.5265\n",
      "Epoch 236/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4325 - acc: 0.5230\n",
      "Epoch 237/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4237 - acc: 0.5349\n",
      "Epoch 238/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4335 - acc: 0.5184\n",
      "Epoch 239/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4440 - acc: 0.5293\n",
      "Epoch 240/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4315 - acc: 0.5309\n",
      "Epoch 241/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4607 - acc: 0.5132\n",
      "Epoch 242/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4536 - acc: 0.5280\n",
      "Epoch 243/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4446 - acc: 0.5253\n",
      "Epoch 244/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4342 - acc: 0.5293\n",
      "Epoch 245/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4531 - acc: 0.5241\n",
      "Epoch 246/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4257 - acc: 0.5332\n",
      "Epoch 247/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4256 - acc: 0.5385\n",
      "Epoch 248/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4474 - acc: 0.5195\n",
      "Epoch 249/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4603 - acc: 0.5164\n",
      "Epoch 250/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4406 - acc: 0.5201\n",
      "Epoch 251/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4422 - acc: 0.5297\n",
      "Epoch 252/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4396 - acc: 0.5305\n",
      "Epoch 253/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4409 - acc: 0.5284\n",
      "Epoch 254/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4256 - acc: 0.5341\n",
      "Epoch 255/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4337 - acc: 0.5290\n",
      "Epoch 256/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4500 - acc: 0.5188\n",
      "Epoch 257/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4120 - acc: 0.5429\n",
      "Epoch 258/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4298 - acc: 0.5329\n",
      "Epoch 259/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4427 - acc: 0.5221\n",
      "Epoch 260/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4115 - acc: 0.5377\n",
      "Epoch 261/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4365 - acc: 0.5177\n",
      "Epoch 262/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4276 - acc: 0.5337\n",
      "Epoch 263/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4165 - acc: 0.5266\n",
      "Epoch 264/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4176 - acc: 0.5276\n",
      "Epoch 265/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4233 - acc: 0.5349\n",
      "Epoch 266/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4218 - acc: 0.5268\n",
      "Epoch 267/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4409 - acc: 0.5225\n",
      "Epoch 268/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4454 - acc: 0.5245\n",
      "Epoch 269/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4246 - acc: 0.5312\n",
      "Epoch 270/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4410 - acc: 0.5220\n",
      "Epoch 271/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4728 - acc: 0.5133\n",
      "Epoch 272/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4257 - acc: 0.5353\n",
      "Epoch 273/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4268 - acc: 0.5296\n",
      "Epoch 274/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4399 - acc: 0.5365\n",
      "Epoch 275/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4524 - acc: 0.5274\n",
      "Epoch 276/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4266 - acc: 0.5204\n",
      "Epoch 277/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4157 - acc: 0.5325\n",
      "Epoch 278/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4318 - acc: 0.5244\n",
      "Epoch 279/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4351 - acc: 0.5241\n",
      "Epoch 280/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3975 - acc: 0.5365\n",
      "Epoch 281/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4245 - acc: 0.5264\n",
      "Epoch 282/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4404 - acc: 0.5316\n",
      "Epoch 283/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4217 - acc: 0.5358\n",
      "Epoch 284/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4180 - acc: 0.5365\n",
      "Epoch 285/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4138 - acc: 0.5304\n",
      "Epoch 286/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4165 - acc: 0.5245\n",
      "Epoch 287/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4061 - acc: 0.5361\n",
      "Epoch 288/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4322 - acc: 0.5313\n",
      "Epoch 289/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4209 - acc: 0.5300\n",
      "Epoch 290/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4204 - acc: 0.5316\n",
      "Epoch 291/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3990 - acc: 0.5397\n",
      "Epoch 292/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4246 - acc: 0.5225\n",
      "Epoch 293/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4313 - acc: 0.5304\n",
      "Epoch 294/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4374 - acc: 0.5232\n",
      "Epoch 295/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4083 - acc: 0.5337\n",
      "Epoch 296/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4292 - acc: 0.5321\n",
      "Epoch 297/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4131 - acc: 0.5341\n",
      "Epoch 298/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4238 - acc: 0.5401\n",
      "Epoch 299/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4135 - acc: 0.5285\n",
      "Epoch 300/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4059 - acc: 0.5341\n",
      "Epoch 301/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4238 - acc: 0.5369\n",
      "Epoch 302/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4489 - acc: 0.5197\n",
      "Epoch 303/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4225 - acc: 0.5336\n",
      "Epoch 304/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3945 - acc: 0.5345\n",
      "Epoch 305/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4175 - acc: 0.5325\n",
      "Epoch 306/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4186 - acc: 0.5296\n",
      "Epoch 307/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3981 - acc: 0.5320\n",
      "Epoch 308/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4307 - acc: 0.5381\n",
      "Epoch 309/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4181 - acc: 0.5329\n",
      "Epoch 310/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4086 - acc: 0.5349\n",
      "Epoch 311/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4011 - acc: 0.5305\n",
      "Epoch 312/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4090 - acc: 0.5373\n",
      "Epoch 313/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4030 - acc: 0.5357\n",
      "Epoch 314/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3901 - acc: 0.5462\n",
      "Epoch 315/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3883 - acc: 0.5440\n",
      "Epoch 316/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4207 - acc: 0.5292\n",
      "Epoch 317/1000\n",
      "39/38 [==============================] - 2s 38ms/step - loss: 1.4198 - acc: 0.5197\n",
      "Epoch 318/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4244 - acc: 0.5329\n",
      "Epoch 319/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4139 - acc: 0.5345\n",
      "Epoch 320/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4185 - acc: 0.5349\n",
      "Epoch 321/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4047 - acc: 0.5405\n",
      "Epoch 322/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4138 - acc: 0.5304\n",
      "Epoch 323/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4173 - acc: 0.5301\n",
      "Epoch 324/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4379 - acc: 0.5193\n",
      "Epoch 325/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4090 - acc: 0.5296\n",
      "Epoch 326/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4172 - acc: 0.5257\n",
      "Epoch 327/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4189 - acc: 0.5281\n",
      "Epoch 328/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4179 - acc: 0.5389\n",
      "Epoch 329/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4042 - acc: 0.5424\n",
      "Epoch 330/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4188 - acc: 0.5288\n",
      "Epoch 331/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4272 - acc: 0.5353\n",
      "Epoch 332/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4121 - acc: 0.5353\n",
      "Epoch 333/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3891 - acc: 0.5493\n",
      "Epoch 334/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4199 - acc: 0.5302\n",
      "Epoch 335/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4001 - acc: 0.5368\n",
      "Epoch 336/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4039 - acc: 0.5413\n",
      "Epoch 337/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4095 - acc: 0.5437\n",
      "Epoch 338/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4061 - acc: 0.5408\n",
      "Epoch 339/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3988 - acc: 0.5380\n",
      "Epoch 340/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4073 - acc: 0.5325\n",
      "Epoch 341/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4026 - acc: 0.5377\n",
      "Epoch 342/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3962 - acc: 0.5341\n",
      "Epoch 343/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4198 - acc: 0.5357\n",
      "Epoch 344/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4081 - acc: 0.5345\n",
      "Epoch 345/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4134 - acc: 0.5393\n",
      "Epoch 346/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3873 - acc: 0.5405\n",
      "Epoch 347/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3967 - acc: 0.5373\n",
      "Epoch 348/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4171 - acc: 0.5373\n",
      "Epoch 349/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3971 - acc: 0.5312\n",
      "Epoch 350/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4036 - acc: 0.5465\n",
      "Epoch 351/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3879 - acc: 0.5337\n",
      "Epoch 352/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4174 - acc: 0.5333\n",
      "Epoch 353/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4076 - acc: 0.5356\n",
      "Epoch 354/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4096 - acc: 0.5320\n",
      "Epoch 355/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3875 - acc: 0.5432\n",
      "Epoch 356/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4037 - acc: 0.5396\n",
      "Epoch 357/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4030 - acc: 0.5377\n",
      "Epoch 358/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4111 - acc: 0.5321\n",
      "Epoch 359/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4283 - acc: 0.5393\n",
      "Epoch 360/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4049 - acc: 0.5401\n",
      "Epoch 361/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4007 - acc: 0.5410\n",
      "Epoch 362/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3910 - acc: 0.5426\n",
      "Epoch 363/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4310 - acc: 0.5244\n",
      "Epoch 364/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4116 - acc: 0.5300\n",
      "Epoch 365/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3922 - acc: 0.5377\n",
      "Epoch 366/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4095 - acc: 0.5376\n",
      "Epoch 367/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3966 - acc: 0.5417\n",
      "Epoch 368/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4210 - acc: 0.5293\n",
      "Epoch 369/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4010 - acc: 0.5313\n",
      "Epoch 370/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3872 - acc: 0.5529\n",
      "Epoch 371/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4120 - acc: 0.5397\n",
      "Epoch 372/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4048 - acc: 0.5417\n",
      "Epoch 373/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4058 - acc: 0.5417\n",
      "Epoch 374/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4097 - acc: 0.5437\n",
      "Epoch 375/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4006 - acc: 0.5329\n",
      "Epoch 376/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3791 - acc: 0.5412\n",
      "Epoch 377/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4049 - acc: 0.5413\n",
      "Epoch 378/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3897 - acc: 0.5461\n",
      "Epoch 379/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4018 - acc: 0.5385\n",
      "Epoch 380/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4139 - acc: 0.5345\n",
      "Epoch 381/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3927 - acc: 0.5336\n",
      "Epoch 382/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4270 - acc: 0.5240\n",
      "Epoch 383/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4092 - acc: 0.5360\n",
      "Epoch 384/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4063 - acc: 0.5361\n",
      "Epoch 385/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4067 - acc: 0.5272\n",
      "Epoch 386/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3984 - acc: 0.5313\n",
      "Epoch 387/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4037 - acc: 0.5300\n",
      "Epoch 388/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4293 - acc: 0.5329\n",
      "Epoch 389/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3942 - acc: 0.5413\n",
      "Epoch 390/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4054 - acc: 0.5320\n",
      "Epoch 391/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3977 - acc: 0.5365\n",
      "Epoch 392/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3882 - acc: 0.5446\n",
      "Epoch 393/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3969 - acc: 0.5425\n",
      "Epoch 394/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3930 - acc: 0.5458\n",
      "Epoch 395/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3854 - acc: 0.5450\n",
      "Epoch 396/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4071 - acc: 0.5321\n",
      "Epoch 397/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4199 - acc: 0.5405\n",
      "Epoch 398/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3882 - acc: 0.5433\n",
      "Epoch 399/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3910 - acc: 0.5400\n",
      "Epoch 400/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3754 - acc: 0.5522\n",
      "Epoch 401/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4080 - acc: 0.5394\n",
      "Epoch 402/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3750 - acc: 0.5394\n",
      "Epoch 403/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4063 - acc: 0.5349\n",
      "Epoch 404/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3902 - acc: 0.5453\n",
      "Epoch 405/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3815 - acc: 0.5397\n",
      "Epoch 406/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3966 - acc: 0.5441\n",
      "Epoch 407/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3969 - acc: 0.5389\n",
      "Epoch 408/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4054 - acc: 0.5341\n",
      "Epoch 409/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4111 - acc: 0.5313\n",
      "Epoch 410/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4293 - acc: 0.5252\n",
      "Epoch 411/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4023 - acc: 0.5328\n",
      "Epoch 412/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3982 - acc: 0.5336\n",
      "Epoch 413/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4041 - acc: 0.5322\n",
      "Epoch 414/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4014 - acc: 0.5331\n",
      "Epoch 415/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3880 - acc: 0.5465\n",
      "Epoch 416/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3861 - acc: 0.5441\n",
      "Epoch 417/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4097 - acc: 0.5365\n",
      "Epoch 418/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4044 - acc: 0.5357\n",
      "Epoch 419/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4037 - acc: 0.5316\n",
      "Epoch 420/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3597 - acc: 0.5545\n",
      "Epoch 421/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4276 - acc: 0.5321\n",
      "Epoch 422/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4049 - acc: 0.5389\n",
      "Epoch 423/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3957 - acc: 0.5344\n",
      "Epoch 424/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3956 - acc: 0.5453\n",
      "Epoch 425/1000\n",
      "39/38 [==============================] - 2s 38ms/step - loss: 1.3983 - acc: 0.5417\n",
      "Epoch 426/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4031 - acc: 0.5341\n",
      "Epoch 427/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3999 - acc: 0.5425\n",
      "Epoch 428/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3894 - acc: 0.5437\n",
      "Epoch 429/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4110 - acc: 0.5280\n",
      "Epoch 430/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3949 - acc: 0.5369\n",
      "Epoch 431/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4033 - acc: 0.5401\n",
      "Epoch 432/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3753 - acc: 0.5465\n",
      "Epoch 433/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3910 - acc: 0.5446\n",
      "Epoch 434/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3842 - acc: 0.5405\n",
      "Epoch 435/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4275 - acc: 0.5268\n",
      "Epoch 436/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4084 - acc: 0.5377\n",
      "Epoch 437/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3896 - acc: 0.5429\n",
      "Epoch 438/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3979 - acc: 0.5373\n",
      "Epoch 439/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4013 - acc: 0.5346\n",
      "Epoch 440/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4098 - acc: 0.5264\n",
      "Epoch 441/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3975 - acc: 0.5349\n",
      "Epoch 442/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4139 - acc: 0.5378\n",
      "Epoch 443/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3938 - acc: 0.5357\n",
      "Epoch 444/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4186 - acc: 0.5350\n",
      "Epoch 445/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3963 - acc: 0.5365\n",
      "Epoch 446/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4017 - acc: 0.5357\n",
      "Epoch 447/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3923 - acc: 0.5389\n",
      "Epoch 448/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3979 - acc: 0.5405\n",
      "Epoch 449/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3990 - acc: 0.5416\n",
      "Epoch 450/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3864 - acc: 0.5357\n",
      "Epoch 451/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3959 - acc: 0.5389\n",
      "Epoch 452/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3960 - acc: 0.5393\n",
      "Epoch 453/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3873 - acc: 0.5317\n",
      "Epoch 454/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4025 - acc: 0.5340\n",
      "Epoch 455/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3721 - acc: 0.5517\n",
      "Epoch 456/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3703 - acc: 0.5474\n",
      "Epoch 457/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3792 - acc: 0.5493\n",
      "Epoch 458/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.4032 - acc: 0.5409\n",
      "Epoch 459/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3874 - acc: 0.5393\n",
      "Epoch 460/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3937 - acc: 0.5304\n",
      "Epoch 461/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3936 - acc: 0.5525\n",
      "Epoch 462/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3838 - acc: 0.5437\n",
      "Epoch 463/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3711 - acc: 0.5481\n",
      "Epoch 464/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4013 - acc: 0.5397\n",
      "Epoch 465/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4180 - acc: 0.5413\n",
      "Epoch 466/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3980 - acc: 0.5422\n",
      "Epoch 467/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3956 - acc: 0.5364\n",
      "Epoch 468/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3754 - acc: 0.5437\n",
      "Epoch 469/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3896 - acc: 0.5357\n",
      "Epoch 470/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3884 - acc: 0.5424\n",
      "Epoch 471/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3782 - acc: 0.5454\n",
      "Epoch 472/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3912 - acc: 0.5453\n",
      "Epoch 473/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3973 - acc: 0.5396\n",
      "Epoch 474/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4007 - acc: 0.5409\n",
      "Epoch 475/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3829 - acc: 0.5408\n",
      "Epoch 476/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3794 - acc: 0.5353\n",
      "Epoch 477/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3886 - acc: 0.5384\n",
      "Epoch 478/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3884 - acc: 0.5440\n",
      "Epoch 479/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3991 - acc: 0.5334\n",
      "Epoch 480/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3907 - acc: 0.5400\n",
      "Epoch 481/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3735 - acc: 0.5429\n",
      "Epoch 482/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4036 - acc: 0.5353\n",
      "Epoch 483/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3891 - acc: 0.5461\n",
      "Epoch 484/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4012 - acc: 0.5386\n",
      "Epoch 485/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3991 - acc: 0.5376\n",
      "Epoch 486/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4019 - acc: 0.5429\n",
      "Epoch 487/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3769 - acc: 0.5394\n",
      "Epoch 488/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3875 - acc: 0.5472\n",
      "Epoch 489/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3932 - acc: 0.5313\n",
      "Epoch 490/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3988 - acc: 0.5373\n",
      "Epoch 491/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3910 - acc: 0.5360\n",
      "Epoch 492/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4070 - acc: 0.5429\n",
      "Epoch 493/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3926 - acc: 0.5333\n",
      "Epoch 494/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3956 - acc: 0.5365\n",
      "Epoch 495/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3676 - acc: 0.5469\n",
      "Epoch 496/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3928 - acc: 0.5421\n",
      "Epoch 497/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3710 - acc: 0.5442\n",
      "Epoch 498/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3939 - acc: 0.5358\n",
      "Epoch 499/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3952 - acc: 0.5381\n",
      "Epoch 500/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3852 - acc: 0.5469\n",
      "Epoch 501/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3937 - acc: 0.5473\n",
      "Epoch 502/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4048 - acc: 0.5437\n",
      "Epoch 503/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4042 - acc: 0.5485\n",
      "Epoch 504/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3739 - acc: 0.5457\n",
      "Epoch 505/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3724 - acc: 0.5493\n",
      "Epoch 506/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3982 - acc: 0.5433\n",
      "Epoch 507/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3967 - acc: 0.5384\n",
      "Epoch 508/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3943 - acc: 0.5361\n",
      "Epoch 509/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3893 - acc: 0.5449\n",
      "Epoch 510/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3926 - acc: 0.5360\n",
      "Epoch 511/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3778 - acc: 0.5449\n",
      "Epoch 512/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3933 - acc: 0.5401\n",
      "Epoch 513/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3806 - acc: 0.5445\n",
      "Epoch 514/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3913 - acc: 0.5357\n",
      "Epoch 515/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3975 - acc: 0.5337\n",
      "Epoch 516/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3945 - acc: 0.5398\n",
      "Epoch 517/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3937 - acc: 0.5304\n",
      "Epoch 518/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3934 - acc: 0.5381\n",
      "Epoch 519/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3990 - acc: 0.5302\n",
      "Epoch 520/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3707 - acc: 0.5481\n",
      "Epoch 521/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3875 - acc: 0.5474\n",
      "Epoch 522/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4055 - acc: 0.5384\n",
      "Epoch 523/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3838 - acc: 0.5441\n",
      "Epoch 524/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3858 - acc: 0.5440\n",
      "Epoch 525/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3904 - acc: 0.5421\n",
      "Epoch 526/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3767 - acc: 0.5500\n",
      "Epoch 527/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3692 - acc: 0.5425\n",
      "Epoch 528/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3810 - acc: 0.5482\n",
      "Epoch 529/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3844 - acc: 0.5433\n",
      "Epoch 530/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3773 - acc: 0.5477\n",
      "Epoch 531/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3808 - acc: 0.5417\n",
      "Epoch 532/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3912 - acc: 0.5365\n",
      "Epoch 533/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3741 - acc: 0.5473\n",
      "Epoch 534/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3897 - acc: 0.5410\n",
      "Epoch 535/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3948 - acc: 0.5402\n",
      "Epoch 536/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3774 - acc: 0.5392\n",
      "Epoch 537/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3882 - acc: 0.5438\n",
      "Epoch 538/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3890 - acc: 0.5378\n",
      "Epoch 539/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3762 - acc: 0.5446\n",
      "Epoch 540/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3785 - acc: 0.5364\n",
      "Epoch 541/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3942 - acc: 0.5413\n",
      "Epoch 542/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3748 - acc: 0.5453\n",
      "Epoch 543/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3992 - acc: 0.5325\n",
      "Epoch 544/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3985 - acc: 0.5441\n",
      "Epoch 545/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3861 - acc: 0.5408\n",
      "Epoch 546/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3721 - acc: 0.5469\n",
      "Epoch 547/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3747 - acc: 0.5442\n",
      "Epoch 548/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3922 - acc: 0.5385\n",
      "Epoch 549/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3910 - acc: 0.5421\n",
      "Epoch 550/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3799 - acc: 0.5405\n",
      "Epoch 551/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3890 - acc: 0.5360\n",
      "Epoch 552/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3939 - acc: 0.5416\n",
      "Epoch 553/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3627 - acc: 0.5393\n",
      "Epoch 554/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3912 - acc: 0.5409\n",
      "Epoch 555/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3691 - acc: 0.5473\n",
      "Epoch 556/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3906 - acc: 0.5341\n",
      "Epoch 557/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3915 - acc: 0.5470\n",
      "Epoch 558/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3770 - acc: 0.5413\n",
      "Epoch 559/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3839 - acc: 0.5466\n",
      "Epoch 560/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3911 - acc: 0.5541\n",
      "Epoch 561/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3680 - acc: 0.5597\n",
      "Epoch 562/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3823 - acc: 0.5388\n",
      "Epoch 563/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3849 - acc: 0.5436\n",
      "Epoch 564/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3805 - acc: 0.5408\n",
      "Epoch 565/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3936 - acc: 0.5428\n",
      "Epoch 566/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3556 - acc: 0.5513\n",
      "Epoch 567/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3939 - acc: 0.5353\n",
      "Epoch 568/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3851 - acc: 0.5421\n",
      "Epoch 569/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3640 - acc: 0.5489\n",
      "Epoch 570/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3684 - acc: 0.5426\n",
      "Epoch 571/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3935 - acc: 0.5417\n",
      "Epoch 572/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3754 - acc: 0.5473\n",
      "Epoch 573/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3892 - acc: 0.5434\n",
      "Epoch 574/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3770 - acc: 0.5434\n",
      "Epoch 575/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3581 - acc: 0.5561\n",
      "Epoch 576/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3799 - acc: 0.5445\n",
      "Epoch 577/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3870 - acc: 0.5450\n",
      "Epoch 578/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3776 - acc: 0.5449\n",
      "Epoch 579/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3764 - acc: 0.5452\n",
      "Epoch 580/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3883 - acc: 0.5408\n",
      "Epoch 581/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3684 - acc: 0.5501\n",
      "Epoch 582/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4084 - acc: 0.5453\n",
      "Epoch 583/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3871 - acc: 0.5465\n",
      "Epoch 584/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3749 - acc: 0.5449\n",
      "Epoch 585/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3861 - acc: 0.5377\n",
      "Epoch 586/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3951 - acc: 0.5373\n",
      "Epoch 587/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3658 - acc: 0.5493\n",
      "Epoch 588/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3488 - acc: 0.5549\n",
      "Epoch 589/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3961 - acc: 0.5361\n",
      "Epoch 590/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3702 - acc: 0.5481\n",
      "Epoch 591/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3667 - acc: 0.5510\n",
      "Epoch 592/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3718 - acc: 0.5473\n",
      "Epoch 593/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3789 - acc: 0.5446\n",
      "Epoch 594/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3867 - acc: 0.5389\n",
      "Epoch 595/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3887 - acc: 0.5373\n",
      "Epoch 596/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3894 - acc: 0.5425\n",
      "Epoch 597/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3649 - acc: 0.5505\n",
      "Epoch 598/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3766 - acc: 0.5481\n",
      "Epoch 599/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3722 - acc: 0.5373\n",
      "Epoch 600/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3775 - acc: 0.5462\n",
      "Epoch 601/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3776 - acc: 0.5418\n",
      "Epoch 602/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3648 - acc: 0.5514\n",
      "Epoch 603/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3763 - acc: 0.5428\n",
      "Epoch 604/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3786 - acc: 0.5529\n",
      "Epoch 605/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3612 - acc: 0.5485\n",
      "Epoch 606/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3786 - acc: 0.5388\n",
      "Epoch 607/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3803 - acc: 0.5405\n",
      "Epoch 608/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3780 - acc: 0.5528\n",
      "Epoch 609/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3857 - acc: 0.5457\n",
      "Epoch 610/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3851 - acc: 0.5373\n",
      "Epoch 611/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3693 - acc: 0.5421\n",
      "Epoch 612/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3599 - acc: 0.5534\n",
      "Epoch 613/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3826 - acc: 0.5490\n",
      "Epoch 614/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3765 - acc: 0.5449\n",
      "Epoch 615/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3791 - acc: 0.5501\n",
      "Epoch 616/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3677 - acc: 0.5578\n",
      "Epoch 617/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3711 - acc: 0.5465\n",
      "Epoch 618/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3692 - acc: 0.5481\n",
      "Epoch 619/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3361 - acc: 0.5618\n",
      "Epoch 620/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3791 - acc: 0.5469\n",
      "Epoch 621/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3792 - acc: 0.5461\n",
      "Epoch 622/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3946 - acc: 0.5401\n",
      "Epoch 623/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3903 - acc: 0.5373\n",
      "Epoch 624/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3587 - acc: 0.5493\n",
      "Epoch 625/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3687 - acc: 0.5445\n",
      "Epoch 626/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3785 - acc: 0.5437\n",
      "Epoch 627/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3818 - acc: 0.5389\n",
      "Epoch 628/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3900 - acc: 0.5393\n",
      "Epoch 629/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3684 - acc: 0.5494\n",
      "Epoch 630/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3875 - acc: 0.5412\n",
      "Epoch 631/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3742 - acc: 0.5457\n",
      "Epoch 632/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3838 - acc: 0.5373\n",
      "Epoch 633/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3827 - acc: 0.5481\n",
      "Epoch 634/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3748 - acc: 0.5522\n",
      "Epoch 635/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3912 - acc: 0.5493\n",
      "Epoch 636/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3754 - acc: 0.5409\n",
      "Epoch 637/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3688 - acc: 0.5476\n",
      "Epoch 638/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3812 - acc: 0.5461\n",
      "Epoch 639/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3742 - acc: 0.5537\n",
      "Epoch 640/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3697 - acc: 0.5454\n",
      "Epoch 641/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3892 - acc: 0.5400\n",
      "Epoch 642/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3764 - acc: 0.5450\n",
      "Epoch 643/1000\n",
      "39/38 [==============================] - 2s 38ms/step - loss: 1.3855 - acc: 0.5453\n",
      "Epoch 644/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3846 - acc: 0.5442\n",
      "Epoch 645/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3742 - acc: 0.5490\n",
      "Epoch 646/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3707 - acc: 0.5445\n",
      "Epoch 647/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3725 - acc: 0.5425\n",
      "Epoch 648/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3589 - acc: 0.5553\n",
      "Epoch 649/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3541 - acc: 0.5517\n",
      "Epoch 650/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3701 - acc: 0.5458\n",
      "Epoch 651/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3724 - acc: 0.5401\n",
      "Epoch 652/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.4119 - acc: 0.5321\n",
      "Epoch 653/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3800 - acc: 0.5441\n",
      "Epoch 654/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3781 - acc: 0.5332\n",
      "Epoch 655/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3659 - acc: 0.5397\n",
      "Epoch 656/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3889 - acc: 0.5406\n",
      "Epoch 657/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3792 - acc: 0.5449\n",
      "Epoch 658/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3664 - acc: 0.5437\n",
      "Epoch 659/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3594 - acc: 0.5498\n",
      "Epoch 660/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3822 - acc: 0.5420\n",
      "Epoch 661/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3815 - acc: 0.5469\n",
      "Epoch 662/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3941 - acc: 0.5446\n",
      "Epoch 663/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3848 - acc: 0.5405\n",
      "Epoch 664/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3749 - acc: 0.5429\n",
      "Epoch 665/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3758 - acc: 0.5488\n",
      "Epoch 666/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3556 - acc: 0.5513\n",
      "Epoch 667/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3626 - acc: 0.5462\n",
      "Epoch 668/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3873 - acc: 0.5349\n",
      "Epoch 669/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3600 - acc: 0.5505\n",
      "Epoch 670/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3680 - acc: 0.5377\n",
      "Epoch 671/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3686 - acc: 0.5473\n",
      "Epoch 672/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3820 - acc: 0.5306\n",
      "Epoch 673/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3701 - acc: 0.5456\n",
      "Epoch 674/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3737 - acc: 0.5440\n",
      "Epoch 675/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3792 - acc: 0.5465\n",
      "Epoch 676/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3645 - acc: 0.5533\n",
      "Epoch 677/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3716 - acc: 0.5449\n",
      "Epoch 678/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3489 - acc: 0.5533\n",
      "Epoch 679/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3808 - acc: 0.5433\n",
      "Epoch 680/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3634 - acc: 0.5450\n",
      "Epoch 681/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3705 - acc: 0.5517\n",
      "Epoch 682/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3615 - acc: 0.5538\n",
      "Epoch 683/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3683 - acc: 0.5554\n",
      "Epoch 684/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3666 - acc: 0.5474\n",
      "Epoch 685/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3884 - acc: 0.5361\n",
      "Epoch 686/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3738 - acc: 0.5380\n",
      "Epoch 687/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3831 - acc: 0.5377\n",
      "Epoch 688/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3723 - acc: 0.5469\n",
      "Epoch 689/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3774 - acc: 0.5445\n",
      "Epoch 690/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3560 - acc: 0.5497\n",
      "Epoch 691/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3726 - acc: 0.5493\n",
      "Epoch 692/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3622 - acc: 0.5493\n",
      "Epoch 693/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3822 - acc: 0.5397\n",
      "Epoch 694/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3738 - acc: 0.5461\n",
      "Epoch 695/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.4003 - acc: 0.5341\n",
      "Epoch 696/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3682 - acc: 0.5481\n",
      "Epoch 697/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3626 - acc: 0.5473\n",
      "Epoch 698/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3675 - acc: 0.5578\n",
      "Epoch 699/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3542 - acc: 0.5446\n",
      "Epoch 700/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3789 - acc: 0.5458\n",
      "Epoch 701/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3676 - acc: 0.5505\n",
      "Epoch 702/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3833 - acc: 0.5457\n",
      "Epoch 703/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3516 - acc: 0.5522\n",
      "Epoch 704/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3461 - acc: 0.5592\n",
      "Epoch 705/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3699 - acc: 0.5481\n",
      "Epoch 706/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3854 - acc: 0.5489\n",
      "Epoch 707/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3725 - acc: 0.5474\n",
      "Epoch 708/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3733 - acc: 0.5518\n",
      "Epoch 709/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3660 - acc: 0.5529\n",
      "Epoch 710/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3633 - acc: 0.5538\n",
      "Epoch 711/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3824 - acc: 0.5390\n",
      "Epoch 712/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3571 - acc: 0.5538\n",
      "Epoch 713/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3555 - acc: 0.5516\n",
      "Epoch 714/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3800 - acc: 0.5393\n",
      "Epoch 715/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3796 - acc: 0.5429\n",
      "Epoch 716/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3705 - acc: 0.5462\n",
      "Epoch 717/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3694 - acc: 0.5448\n",
      "Epoch 718/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3662 - acc: 0.5481\n",
      "Epoch 719/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3735 - acc: 0.5441\n",
      "Epoch 720/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3586 - acc: 0.5469\n",
      "Epoch 721/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3586 - acc: 0.5541\n",
      "Epoch 722/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3706 - acc: 0.5561\n",
      "Epoch 723/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3864 - acc: 0.5505\n",
      "Epoch 724/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3556 - acc: 0.5521\n",
      "Epoch 725/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3627 - acc: 0.5461\n",
      "Epoch 726/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3849 - acc: 0.5409\n",
      "Epoch 727/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3559 - acc: 0.5448\n",
      "Epoch 728/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3680 - acc: 0.5477\n",
      "Epoch 729/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3970 - acc: 0.5349\n",
      "Epoch 730/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3791 - acc: 0.5469\n",
      "Epoch 731/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3733 - acc: 0.5457\n",
      "Epoch 732/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3403 - acc: 0.5549\n",
      "Epoch 733/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3728 - acc: 0.5445\n",
      "Epoch 734/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3761 - acc: 0.5425\n",
      "Epoch 735/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3846 - acc: 0.5477\n",
      "Epoch 736/1000\n",
      "39/38 [==============================] - 2s 40ms/step - loss: 1.3639 - acc: 0.5473\n",
      "Epoch 737/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3801 - acc: 0.5445\n",
      "Epoch 738/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3681 - acc: 0.5409\n",
      "Epoch 739/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3673 - acc: 0.5512\n",
      "Epoch 740/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3794 - acc: 0.5513\n",
      "Epoch 741/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3744 - acc: 0.5492\n",
      "Epoch 742/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3843 - acc: 0.5438\n",
      "Epoch 743/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3613 - acc: 0.5453\n",
      "Epoch 744/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3782 - acc: 0.5454\n",
      "Epoch 745/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3840 - acc: 0.5465\n",
      "Epoch 746/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3591 - acc: 0.5494\n",
      "Epoch 747/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3733 - acc: 0.5478\n",
      "Epoch 748/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3409 - acc: 0.5626\n",
      "Epoch 749/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3697 - acc: 0.5518\n",
      "Epoch 750/1000\n",
      "39/38 [==============================] - 2s 39ms/step - loss: 1.3647 - acc: 0.5481\n",
      "Epoch 751/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3740 - acc: 0.5493\n",
      "Epoch 752/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3741 - acc: 0.5445\n",
      "Epoch 753/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3618 - acc: 0.5458\n",
      "Epoch 754/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3490 - acc: 0.5545\n",
      "Epoch 755/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3764 - acc: 0.5378\n",
      "Epoch 756/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3628 - acc: 0.5577\n",
      "Epoch 757/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3678 - acc: 0.5565\n",
      "Epoch 758/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3715 - acc: 0.5509\n",
      "Epoch 759/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3532 - acc: 0.5533\n",
      "Epoch 760/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3712 - acc: 0.5476\n",
      "Epoch 761/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3627 - acc: 0.5550\n",
      "Epoch 762/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3478 - acc: 0.5549\n",
      "Epoch 763/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3542 - acc: 0.5514\n",
      "Epoch 764/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3546 - acc: 0.5617\n",
      "Epoch 765/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3786 - acc: 0.5425\n",
      "Epoch 766/1000\n",
      "39/38 [==============================] - 1s 37ms/step - loss: 1.3781 - acc: 0.5397\n",
      "Epoch 767/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3554 - acc: 0.5578\n",
      "Epoch 768/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3511 - acc: 0.5457\n",
      "Epoch 769/1000\n",
      "39/38 [==============================] - 1s 38ms/step - loss: 1.3514 - acc: 0.5473\n",
      "Epoch 770/1000\n",
      "23/38 [================>.............] - ETA: 0s - loss: 1.3517 - acc: 0.5618"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-f347d50f3882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#                        verbose=verbose)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m history = cnn_model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n\u001b[0;32m---> 24\u001b[0;31m                     steps_per_epoch=len(X_train) / batch_size, epochs=epoch_n)\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_n = 1000\n",
    "batch_size = 64\n",
    "verbose = 1\n",
    "file_name = 'cnn_3CN1D_noMAxPooling_bs'+str(batch_size)\n",
    "model_name_wrt = 'models/cnn/' + file_name+'.hdf5' \n",
    "\n",
    "# Callback for Early Stopping... May want to raise the min_delta for small numbers of epochs\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.000001, patience=100,verbose=1, mode='auto', \n",
    "                             restore_best_weights=True)\n",
    "# Callback for Reducing the Learning Rate... when the monitor levels out for 'patience' epochs, then the LR is reduced\n",
    "rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,patience=30, min_lr=1e-6, mode='auto', verbose=1)\n",
    "# Save the best value of the model for future use\n",
    "sv_mod = callbacks.ModelCheckpoint(model_name_wrt, monitor='val_loss', save_best_only=True, period=1)\n",
    "\n",
    "cnn_model = compile_model()\n",
    "# history = cnn_model.fit(X_train, y_train, \n",
    "#                        validation_split=0.1, \n",
    "#                        class_weight=class_weights,\n",
    "#                        callbacks=[es, rlr, sv_mod], \n",
    "#                        epochs=epoch_n, \n",
    "#                        batch_size=batch_size, \n",
    "#                        verbose=verbose)\n",
    "history = cnn_model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                                  \n",
    "                    steps_per_epoch=len(X_train) / batch_size, epochs=epoch_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, 'cnn')\n",
    "\n",
    "best_model = load_model(model_name_wrt)\n",
    "test_predict=best_model.predict(X_test)\n",
    "\n",
    "predicted_classes = test_predict.argmax(1)+1\n",
    "counter=collections.Counter(predicted_classes)\n",
    "print(counter)\n",
    "sns.countplot(predicted_classes, palette='Set3')\n",
    "\n",
    "prob = pd.DataFrame(test_predict)\n",
    "# prob = pd.DataFrame(get_prediction_from_np_array(test_predict))\n",
    "prob.columns = ['crop_id_1', 'crop_id_2', 'crop_id_3', 'crop_id_4', 'crop_id_5', 'crop_id_6', 'crop_id_7','crop_id_8', 'crop_id_9']\n",
    "print(prob.shape)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['field_id'] = sample_sub['field_id']\n",
    "submission = submission.join(prob)\n",
    "submission.head(10)\n",
    "\n",
    "submission.to_csv('predictions/reduced_tabular/'+file_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
