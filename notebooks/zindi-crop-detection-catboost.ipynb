{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import  XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, log_loss, f1_score, accuracy_score, make_scorer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras import optimizers, callbacks \n",
    "\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization,Add,Dropout\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils, plot_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "import collections\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (2494, 10363)\n",
      "test shape (1074, 10362)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/zindi-crop-detection-final-tabular-data/train_final3.csv')\n",
    "test = pd.read_csv('/kaggle/input/zindi-crop-detection-final-tabular-data/test_final3.csv')\n",
    "sample_sub = pd.read_csv('/kaggle/input/zindi-crop-detection-final-tabular-data/sample_submission2.csv')\n",
    "print('train shape {}'.format(train.shape))\n",
    "print('test shape {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1782\n"
     ]
    }
   ],
   "source": [
    "dates = [\"2017-01-01\", \"2017-02-10\", \"2017-06-20\", \n",
    "        \"2017-07-10\", \"2017-07-15\", \"2017-08-04\", \"2017-08-19\",\n",
    "        \"2017-01-31\", \"2017-03-12\", \"2017-03-22\", \"2017-05-31\"]\n",
    "\n",
    "features = ['red', 'green', 'blue', 'band5', 'band6', 'band7', 'nir', 'band8a', 'band11', 'band12',\n",
    "           'ndvi', 'reip', 'datt3', 'gemi', 'msbi', 'ccci', 'avi', 'cvi', 'ndsi', 'siwsi', 'savi', 'evi', 'ndre',\n",
    "            'cari', 'tvi', 'afair21', 'mnsi', 'afri16', 'cri550', 'cri770', 'tci', 'nbr', 'nd776', 'mcari', 'maccioni',\n",
    "            'lci', 'epichla', 'gndvi2', 'gli'\n",
    "           ]\n",
    "\n",
    "columns = []\n",
    "for date in dates:\n",
    "    for layer in features:\n",
    "        columns = columns + [\n",
    "   layer+ '_val1'+date, layer+ '_val2'+date, layer+ '_val3'+date, layer+ '_val4'+date, layer+ '_val5'+date, layer+ '_val6'+date,\n",
    "   layer+ '_val7'+date, layer+ '_val8'+date, layer+ '_val9'+date, layer+ '_val10'+date,\n",
    "   layer+ '_val11'+date, layer+ '_val12'+date, layer+ '_val13'+date, layer+ '_val14'+date, layer+ '_val15'+date, layer+ '_val16'+date,\n",
    "   layer+ '_val17'+date, layer+ '_val18'+date, layer+ '_val19'+date, layer+ '_val20'+date]\n",
    "\n",
    "    \n",
    "train_cols = columns + ['Crop_Id_Ne', 'Field_Id']\n",
    "test_cols = columns + ['Field_Id']\n",
    "X_train = train.drop(columns = train_cols, axis = 1)\n",
    "X_train.reset_index(inplace = True, drop= True)\n",
    "X_test = test.drop(columns = test_cols, axis = 1)\n",
    "X_test.reset_index(inplace = True, drop= True)\n",
    "y_train = train['Crop_Id_Ne']\n",
    "X_train['xy'] = -(X_train['x'] + X_train['y'])/100\n",
    "X_test['xy'] = -(X_test['x'] + X_test['y'])/100\n",
    "\n",
    "print(len(X_train.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source https://www.kaggle.com/super13579/lgbm-model-catboost/notebook\n",
    "def kfold_catboost(train,test,train_labels,params=None,num_class=9,metrics=None, debug=False, num_folds = 5,\n",
    "                   stratified = False, log = False, seed = 1):\n",
    "    \n",
    "    oof_valid = np.zeros((len(train),num_class))\n",
    "    oof_test  = np.zeros((len(test),num_class))    \n",
    "    feat_importance = pd.DataFrame()\n",
    "    cv_scores = [] ## cross validation scores in each kfold and overall cv score\n",
    "    train_scores=[] ## train scores of each model trained on kfold\n",
    "    models = [] ## save the models\n",
    "    \n",
    "    if stratified:\n",
    "        kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "    else:\n",
    "        kf = KFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    for ind, (trn_ind, val_ind) in ( enumerate( kf.split(train,train_labels)) ):\n",
    "        \n",
    "        xtr, xvl = train.loc[trn_ind], train.loc[val_ind]\n",
    "        ytr, yvl = train_labels[trn_ind] , train_labels[val_ind]\n",
    "\n",
    "        \n",
    "#         dtrain=lgb.Dataset(data=X_train,label=y_train, feature_name=X_train.columns)\n",
    "#         dval=lgb.Dataset(data=X_valid,label=y_valid, feature_name=X_valid.columns)\n",
    "#         display(X_valid.shape)\n",
    "        \n",
    " \n",
    "        num_round = 100000\n",
    "        ## Fit the model with early stoping\n",
    "        model = CatBoostClassifier( num_round, task_type='GPU', early_stopping_rounds=1000,**params,)\n",
    "        model.fit(xtr, ytr, eval_set=(xvl, yvl), cat_features=[], use_best_model=True, verbose=500)\n",
    "        \n",
    "        ## predict on validation and test sets\n",
    "        val_pred =model.predict(xvl)\n",
    "        train_pred_prob = model.predict_proba(xtr)\n",
    "        val_pred_prob = model.predict_proba(xvl) \n",
    "        test_pred_prob = model.predict_proba(test)\n",
    "        oof_valid[val_ind,:] += np.array(val_pred_prob)\n",
    "        oof_test += np.array(test_pred_prob)\n",
    "        \n",
    "        \n",
    "        ## Print and calcualte the scores\n",
    "        score_fold_validation = log_loss(yvl, val_pred_prob)\n",
    "        cv_scores.append(score_fold_validation)\n",
    "        score_fold_train = log_loss(ytr, train_pred_prob)\n",
    "        train_scores.append(score_fold_train)\n",
    "        \n",
    "        if log:\n",
    "            print('accuracy_score',accuracy_score(yvl,val_pred))\n",
    "            print('f1 score ', f1_score(yvl, val_pred, average='weighted'))\n",
    "            plot_confusion_matrix(confusion_matrix(val_pred, yvl))\n",
    "    \n",
    "        \n",
    "        ## Calculate feature importance\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = test.columns.values\n",
    "        fold_importance_df[\"importance\"] = model.feature_importances_\n",
    "#         fold_importance_df[\"shap_values\"] = abs(shap.TreeExplainer(clf).shap_values(valid_x)[:,:test_df.shape[1]]).mean(axis=0).T\n",
    "        fold_importance_df[\"fold\"] = ind + 1\n",
    "        feat_importance = pd.concat([feat_importance, fold_importance_df], axis=0)\n",
    "        \n",
    "        \n",
    "        if not debug:\n",
    "            pd.DataFrame(oof_preds).to_csv(\"lgb{:03}_{:.5f}_train_oof.csv\".format(test_df.shape[1], score), index=False)\n",
    "            sub_df = pd.read_csv('../input/sample_submission.csv')\n",
    "            sub_df['TARGET'] = sub_preds\n",
    "            sub_df.to_csv(\"lgb{:03}_{:.5f}.csv\".format(test_df.shape[1], score), index= False)\n",
    "        \n",
    "        # Clear variables\n",
    "        models.append(model)\n",
    "        del model, xtr, xvl, ytr, yvl, fold_importance_df\n",
    "        gc.collect()\n",
    "\n",
    "        \n",
    "        print('Iteration : {} - CV Score : {}'.format(str(ind+1),score_fold_validation))\n",
    "        print('='* 80)\n",
    "        \n",
    "        \n",
    "    oof_test /= num_folds\n",
    "    \n",
    "    end_train_score=np.mean(train_scores)\n",
    "    oof_score=log_loss(train_labels, oof_valid)\n",
    "    cv_scores.append(oof_score)\n",
    "    print(\"ending  training  : train score {} - oof Score {}\".format(str(end_train_score),str(oof_score)))\n",
    "\n",
    "#     display_importances(feat_importance)\n",
    "#     return feat_importance, models, cv_scores, oof_test, \n",
    "    return oof_test, cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "# print(class_weights)\n",
    "class_weights = [1.2, 3, 1.3, 0.9, 0.99, 1.1, 1, 0.8, 1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 2.1752431\ttest: 2.1760639\tbest: 2.1760639 (0)\ttotal: 24.2ms\tremaining: 40m 20s\n",
      "500:\tlearn: 0.9068397\ttest: 1.0044570\tbest: 1.0044570 (500)\ttotal: 3.88s\tremaining: 12m 51s\n",
      "1000:\tlearn: 0.7159442\ttest: 0.8639501\tbest: 0.8639501 (1000)\ttotal: 7.13s\tremaining: 11m 45s\n",
      "1500:\tlearn: 0.6181012\ttest: 0.8038538\tbest: 0.8038518 (1499)\ttotal: 10.3s\tremaining: 11m 18s\n",
      "2000:\tlearn: 0.5524231\ttest: 0.7669875\tbest: 0.7669875 (2000)\ttotal: 13.6s\tremaining: 11m 5s\n",
      "2500:\tlearn: 0.5039421\ttest: 0.7443145\tbest: 0.7442999 (2497)\ttotal: 16.8s\tremaining: 10m 55s\n",
      "3000:\tlearn: 0.4644597\ttest: 0.7285248\tbest: 0.7285248 (2999)\ttotal: 20.1s\tremaining: 10m 48s\n",
      "3500:\tlearn: 0.4324464\ttest: 0.7154818\tbest: 0.7154818 (3500)\ttotal: 23.3s\tremaining: 10m 42s\n",
      "4000:\tlearn: 0.4049949\ttest: 0.7060520\tbest: 0.7059885 (3999)\ttotal: 26.6s\tremaining: 10m 37s\n",
      "4500:\tlearn: 0.3801160\ttest: 0.6984347\tbest: 0.6984347 (4500)\ttotal: 29.8s\tremaining: 10m 32s\n",
      "5000:\tlearn: 0.3579387\ttest: 0.6923818\tbest: 0.6922997 (4996)\ttotal: 33s\tremaining: 10m 27s\n",
      "5500:\tlearn: 0.3375655\ttest: 0.6870032\tbest: 0.6870032 (5500)\ttotal: 36.3s\tremaining: 10m 24s\n",
      "6000:\tlearn: 0.3197400\ttest: 0.6832911\tbest: 0.6832754 (5998)\ttotal: 39.5s\tremaining: 10m 19s\n",
      "6500:\tlearn: 0.3021239\ttest: 0.6796001\tbest: 0.6795176 (6489)\ttotal: 42.7s\tremaining: 10m 14s\n",
      "7000:\tlearn: 0.2869842\ttest: 0.6770321\tbest: 0.6770321 (7000)\ttotal: 46s\tremaining: 10m 11s\n",
      "7500:\tlearn: 0.2730215\ttest: 0.6745302\tbest: 0.6744399 (7492)\ttotal: 49.2s\tremaining: 10m 7s\n",
      "8000:\tlearn: 0.2603697\ttest: 0.6722623\tbest: 0.6722623 (8000)\ttotal: 52.5s\tremaining: 10m 3s\n",
      "8500:\tlearn: 0.2482667\ttest: 0.6699668\tbest: 0.6699460 (8498)\ttotal: 55.7s\tremaining: 9m 59s\n",
      "9000:\tlearn: 0.2370744\ttest: 0.6690242\tbest: 0.6689958 (8995)\ttotal: 58.9s\tremaining: 9m 55s\n",
      "9500:\tlearn: 0.2261566\ttest: 0.6672611\tbest: 0.6671981 (9396)\ttotal: 1m 2s\tremaining: 9m 51s\n",
      "10000:\tlearn: 0.2166026\ttest: 0.6664399\tbest: 0.6664298 (9995)\ttotal: 1m 5s\tremaining: 9m 48s\n",
      "10500:\tlearn: 0.2072864\ttest: 0.6650194\tbest: 0.6649179 (10491)\ttotal: 1m 8s\tremaining: 9m 45s\n",
      "11000:\tlearn: 0.1982337\ttest: 0.6640334\tbest: 0.6639532 (10992)\ttotal: 1m 11s\tremaining: 9m 41s\n",
      "11500:\tlearn: 0.1898980\ttest: 0.6633391\tbest: 0.6633368 (11498)\ttotal: 1m 15s\tremaining: 9m 38s\n",
      "12000:\tlearn: 0.1823236\ttest: 0.6626942\tbest: 0.6625685 (11924)\ttotal: 1m 18s\tremaining: 9m 34s\n",
      "12500:\tlearn: 0.1750578\ttest: 0.6631305\tbest: 0.6625685 (11924)\ttotal: 1m 21s\tremaining: 9m 30s\n",
      "bestTest = 0.6625685121\n",
      "bestIteration = 11924\n",
      "Shrink model to first 11925 iterations.\n",
      "Iteration : 1 - CV Score : 0.5373412114802832\n",
      "================================================================================\n",
      "0:\tlearn: 2.1770018\ttest: 2.1770578\tbest: 2.1770578 (0)\ttotal: 15.8ms\tremaining: 26m 21s\n",
      "500:\tlearn: 0.9227447\ttest: 0.9678643\tbest: 0.9678643 (500)\ttotal: 4.86s\tremaining: 16m 5s\n",
      "1000:\tlearn: 0.7320734\ttest: 0.8235225\tbest: 0.8235225 (1000)\ttotal: 8.51s\tremaining: 14m 1s\n",
      "1500:\tlearn: 0.6330853\ttest: 0.7658502\tbest: 0.7658502 (1500)\ttotal: 11.8s\tremaining: 12m 52s\n",
      "2000:\tlearn: 0.5662851\ttest: 0.7314216\tbest: 0.7314021 (1999)\ttotal: 15s\tremaining: 12m 14s\n",
      "2500:\tlearn: 0.5149945\ttest: 0.7090015\tbest: 0.7090015 (2500)\ttotal: 18.3s\tremaining: 11m 51s\n",
      "3000:\tlearn: 0.4749889\ttest: 0.6922131\tbest: 0.6922131 (3000)\ttotal: 21.5s\tremaining: 11m 33s\n",
      "3500:\tlearn: 0.4400156\ttest: 0.6789213\tbest: 0.6789213 (3500)\ttotal: 24.7s\tremaining: 11m 19s\n",
      "4000:\tlearn: 0.4098167\ttest: 0.6702561\tbest: 0.6702484 (3999)\ttotal: 27.8s\tremaining: 11m 7s\n",
      "4500:\tlearn: 0.3842168\ttest: 0.6619955\tbest: 0.6619955 (4500)\ttotal: 31.1s\tremaining: 10m 59s\n",
      "5000:\tlearn: 0.3609653\ttest: 0.6568771\tbest: 0.6567708 (4994)\ttotal: 34.2s\tremaining: 10m 50s\n",
      "5500:\tlearn: 0.3404661\ttest: 0.6526988\tbest: 0.6526887 (5498)\ttotal: 37.4s\tremaining: 10m 43s\n",
      "6000:\tlearn: 0.3213126\ttest: 0.6501847\tbest: 0.6501847 (6000)\ttotal: 40.7s\tremaining: 10m 37s\n",
      "6500:\tlearn: 0.3043661\ttest: 0.6479699\tbest: 0.6479189 (6494)\ttotal: 44s\tremaining: 10m 32s\n",
      "7000:\tlearn: 0.2891093\ttest: 0.6455883\tbest: 0.6454791 (6988)\ttotal: 47.2s\tremaining: 10m 26s\n",
      "7500:\tlearn: 0.2747800\ttest: 0.6440477\tbest: 0.6440477 (7500)\ttotal: 50.4s\tremaining: 10m 21s\n",
      "8000:\tlearn: 0.2616230\ttest: 0.6430143\tbest: 0.6429070 (7953)\ttotal: 53.6s\tremaining: 10m 15s\n",
      "8500:\tlearn: 0.2492929\ttest: 0.6419764\tbest: 0.6417518 (8493)\ttotal: 56.8s\tremaining: 10m 11s\n",
      "9000:\tlearn: 0.2376268\ttest: 0.6412474\tbest: 0.6411173 (8987)\ttotal: 60s\tremaining: 10m 6s\n",
      "9500:\tlearn: 0.2266887\ttest: 0.6408542\tbest: 0.6403804 (9317)\ttotal: 1m 3s\tremaining: 10m 2s\n",
      "10000:\tlearn: 0.2166341\ttest: 0.6397508\tbest: 0.6396242 (9992)\ttotal: 1m 6s\tremaining: 9m 57s\n",
      "10500:\tlearn: 0.2070548\ttest: 0.6397065\tbest: 0.6394718 (10188)\ttotal: 1m 9s\tremaining: 9m 53s\n",
      "11000:\tlearn: 0.1983752\ttest: 0.6383748\tbest: 0.6382820 (10974)\ttotal: 1m 12s\tremaining: 9m 49s\n",
      "11500:\tlearn: 0.1898556\ttest: 0.6380107\tbest: 0.6378273 (11462)\ttotal: 1m 16s\tremaining: 9m 45s\n",
      "12000:\tlearn: 0.1818013\ttest: 0.6376620\tbest: 0.6376620 (12000)\ttotal: 1m 19s\tremaining: 9m 40s\n",
      "12500:\tlearn: 0.1744657\ttest: 0.6377389\tbest: 0.6374803 (12072)\ttotal: 1m 22s\tremaining: 9m 38s\n",
      "13000:\tlearn: 0.1675247\ttest: 0.6375866\tbest: 0.6372626 (12653)\ttotal: 1m 26s\tremaining: 9m 37s\n",
      "13500:\tlearn: 0.1611268\ttest: 0.6378109\tbest: 0.6372626 (12653)\ttotal: 1m 30s\tremaining: 9m 36s\n",
      "bestTest = 0.6372626179\n",
      "bestIteration = 12653\n",
      "Shrink model to first 12654 iterations.\n",
      "Iteration : 2 - CV Score : 0.526752094243457\n",
      "================================================================================\n",
      "0:\tlearn: 2.1762780\ttest: 2.1770580\tbest: 2.1770580 (0)\ttotal: 16.5ms\tremaining: 27m 26s\n",
      "500:\tlearn: 0.9087632\ttest: 1.0185199\tbest: 1.0185199 (500)\ttotal: 3.91s\tremaining: 12m 56s\n",
      "1000:\tlearn: 0.7118390\ttest: 0.8908276\tbest: 0.8908276 (1000)\ttotal: 7.74s\tremaining: 12m 45s\n",
      "1500:\tlearn: 0.6103228\ttest: 0.8325635\tbest: 0.8325635 (1500)\ttotal: 11.5s\tremaining: 12m 37s\n",
      "2000:\tlearn: 0.5453514\ttest: 0.7973301\tbest: 0.7973301 (2000)\ttotal: 15.3s\tremaining: 12m 29s\n",
      "2500:\tlearn: 0.4965960\ttest: 0.7755901\tbest: 0.7755891 (2498)\ttotal: 19.1s\tremaining: 12m 25s\n",
      "3000:\tlearn: 0.4567936\ttest: 0.7582818\tbest: 0.7582641 (2998)\ttotal: 22.8s\tremaining: 12m 16s\n",
      "3500:\tlearn: 0.4229919\ttest: 0.7449735\tbest: 0.7449608 (3493)\ttotal: 26s\tremaining: 11m 55s\n",
      "4000:\tlearn: 0.3954770\ttest: 0.7358305\tbest: 0.7358305 (4000)\ttotal: 29.1s\tremaining: 11m 38s\n",
      "4500:\tlearn: 0.3709795\ttest: 0.7283035\tbest: 0.7282219 (4494)\ttotal: 32.8s\tremaining: 11m 35s\n",
      "5000:\tlearn: 0.3485550\ttest: 0.7210298\tbest: 0.7210271 (4999)\ttotal: 36.7s\tremaining: 11m 36s\n",
      "5500:\tlearn: 0.3289137\ttest: 0.7163421\tbest: 0.7163421 (5500)\ttotal: 40.4s\tremaining: 11m 33s\n",
      "6000:\tlearn: 0.3109946\ttest: 0.7109019\tbest: 0.7108830 (5997)\ttotal: 44.2s\tremaining: 11m 32s\n",
      "6500:\tlearn: 0.2947890\ttest: 0.7069795\tbest: 0.7069729 (6481)\ttotal: 48s\tremaining: 11m 30s\n",
      "7000:\tlearn: 0.2797017\ttest: 0.7044526\tbest: 0.7043896 (6980)\ttotal: 51.5s\tremaining: 11m 23s\n",
      "7500:\tlearn: 0.2655961\ttest: 0.7018754\tbest: 0.7018714 (7497)\ttotal: 54.7s\tremaining: 11m 15s\n",
      "8000:\tlearn: 0.2521765\ttest: 0.6994837\tbest: 0.6994463 (7997)\ttotal: 58s\tremaining: 11m 7s\n",
      "8500:\tlearn: 0.2391799\ttest: 0.6967599\tbest: 0.6967089 (8497)\ttotal: 1m 1s\tremaining: 11m\n",
      "9000:\tlearn: 0.2274458\ttest: 0.6954829\tbest: 0.6954171 (8995)\ttotal: 1m 4s\tremaining: 10m 56s\n",
      "9500:\tlearn: 0.2169869\ttest: 0.6958300\tbest: 0.6952654 (9037)\ttotal: 1m 8s\tremaining: 10m 50s\n",
      "10000:\tlearn: 0.2068617\ttest: 0.6952656\tbest: 0.6948310 (9712)\ttotal: 1m 11s\tremaining: 10m 44s\n",
      "10500:\tlearn: 0.1976765\ttest: 0.6958929\tbest: 0.6948310 (9712)\ttotal: 1m 14s\tremaining: 10m 38s\n",
      "bestTest = 0.6948309564\n",
      "bestIteration = 9712\n",
      "Shrink model to first 9713 iterations.\n",
      "Iteration : 3 - CV Score : 0.6015596382946056\n",
      "================================================================================\n",
      "0:\tlearn: 2.1766544\ttest: 2.1772149\tbest: 2.1772149 (0)\ttotal: 15.2ms\tremaining: 25m 19s\n",
      "500:\tlearn: 0.9179983\ttest: 0.9746126\tbest: 0.9746126 (500)\ttotal: 3.43s\tremaining: 11m 20s\n",
      "1000:\tlearn: 0.7217976\ttest: 0.8221061\tbest: 0.8221061 (1000)\ttotal: 6.71s\tremaining: 11m 4s\n",
      "1500:\tlearn: 0.6261412\ttest: 0.7567680\tbest: 0.7567680 (1500)\ttotal: 9.98s\tremaining: 10m 54s\n",
      "2000:\tlearn: 0.5622075\ttest: 0.7175006\tbest: 0.7175006 (2000)\ttotal: 13.2s\tremaining: 10m 48s\n",
      "2500:\tlearn: 0.5137610\ttest: 0.6905944\tbest: 0.6905944 (2500)\ttotal: 16.5s\tremaining: 10m 44s\n",
      "3000:\tlearn: 0.4737635\ttest: 0.6707586\tbest: 0.6707586 (3000)\ttotal: 20.3s\tremaining: 10m 57s\n",
      "3500:\tlearn: 0.4403015\ttest: 0.6562786\tbest: 0.6562456 (3499)\ttotal: 24.2s\tremaining: 11m 7s\n",
      "4000:\tlearn: 0.4120785\ttest: 0.6456736\tbest: 0.6456736 (4000)\ttotal: 27.9s\tremaining: 11m 10s\n",
      "4500:\tlearn: 0.3869048\ttest: 0.6364311\tbest: 0.6364311 (4500)\ttotal: 31.3s\tremaining: 11m 4s\n",
      "5000:\tlearn: 0.3637186\ttest: 0.6277331\tbest: 0.6277236 (4998)\ttotal: 34.5s\tremaining: 10m 55s\n",
      "5500:\tlearn: 0.3433646\ttest: 0.6222865\tbest: 0.6222859 (5499)\ttotal: 37.8s\tremaining: 10m 48s\n",
      "6000:\tlearn: 0.3247285\ttest: 0.6168588\tbest: 0.6168181 (5972)\ttotal: 41s\tremaining: 10m 42s\n",
      "6500:\tlearn: 0.3079256\ttest: 0.6127351\tbest: 0.6127351 (6500)\ttotal: 44.2s\tremaining: 10m 36s\n",
      "7000:\tlearn: 0.2921392\ttest: 0.6101245\tbest: 0.6101028 (6995)\ttotal: 47.6s\tremaining: 10m 31s\n",
      "7500:\tlearn: 0.2774011\ttest: 0.6076238\tbest: 0.6073773 (7476)\ttotal: 50.8s\tremaining: 10m 26s\n",
      "8000:\tlearn: 0.2639530\ttest: 0.6050864\tbest: 0.6050772 (7999)\ttotal: 54.1s\tremaining: 10m 21s\n",
      "8500:\tlearn: 0.2512028\ttest: 0.6028924\tbest: 0.6028924 (8500)\ttotal: 57.4s\tremaining: 10m 17s\n",
      "9000:\tlearn: 0.2394065\ttest: 0.6022982\tbest: 0.6022191 (8772)\ttotal: 1m\tremaining: 10m 13s\n",
      "9500:\tlearn: 0.2284239\ttest: 0.5997136\tbest: 0.5996279 (9474)\ttotal: 1m 3s\tremaining: 10m 9s\n",
      "10000:\tlearn: 0.2181724\ttest: 0.5991739\tbest: 0.5989574 (9859)\ttotal: 1m 7s\tremaining: 10m 4s\n",
      "10500:\tlearn: 0.2089178\ttest: 0.5986885\tbest: 0.5984391 (10285)\ttotal: 1m 10s\tremaining: 10m\n",
      "11000:\tlearn: 0.2000258\ttest: 0.5973503\tbest: 0.5971649 (10935)\ttotal: 1m 13s\tremaining: 9m 58s\n",
      "11500:\tlearn: 0.1913071\ttest: 0.5964632\tbest: 0.5964632 (11500)\ttotal: 1m 17s\tremaining: 9m 56s\n",
      "12000:\tlearn: 0.1831044\ttest: 0.5963638\tbest: 0.5962385 (11821)\ttotal: 1m 20s\tremaining: 9m 52s\n",
      "12500:\tlearn: 0.1755309\ttest: 0.5955981\tbest: 0.5955725 (12498)\ttotal: 1m 24s\tremaining: 9m 48s\n",
      "13000:\tlearn: 0.1683830\ttest: 0.5952962\tbest: 0.5948356 (12866)\ttotal: 1m 27s\tremaining: 9m 47s\n",
      "13500:\tlearn: 0.1616920\ttest: 0.5952325\tbest: 0.5948356 (12866)\ttotal: 1m 31s\tremaining: 9m 46s\n",
      "bestTest = 0.5948355914\n",
      "bestIteration = 12866\n",
      "Shrink model to first 12867 iterations.\n",
      "Iteration : 4 - CV Score : 0.505286493135423\n",
      "================================================================================\n",
      "0:\tlearn: 2.1772432\ttest: 2.1774537\tbest: 2.1774537 (0)\ttotal: 16.8ms\tremaining: 28m 4s\n",
      "500:\tlearn: 0.9156546\ttest: 0.9672779\tbest: 0.9672779 (500)\ttotal: 3.95s\tremaining: 13m 4s\n",
      "1000:\tlearn: 0.7250156\ttest: 0.8288997\tbest: 0.8288997 (1000)\ttotal: 7.8s\tremaining: 12m 51s\n",
      "1500:\tlearn: 0.6250055\ttest: 0.7658375\tbest: 0.7658375 (1500)\ttotal: 11.6s\tremaining: 12m 39s\n",
      "2000:\tlearn: 0.5601022\ttest: 0.7297466\tbest: 0.7297466 (2000)\ttotal: 15.4s\tremaining: 12m 36s\n",
      "2500:\tlearn: 0.5105663\ttest: 0.7042234\tbest: 0.7042234 (2500)\ttotal: 19.4s\tremaining: 12m 34s\n",
      "3000:\tlearn: 0.4725012\ttest: 0.6882118\tbest: 0.6882118 (3000)\ttotal: 22.9s\tremaining: 12m 18s\n",
      "3500:\tlearn: 0.4385507\ttest: 0.6744910\tbest: 0.6744910 (3500)\ttotal: 26.1s\tremaining: 11m 58s\n",
      "4000:\tlearn: 0.4096459\ttest: 0.6632557\tbest: 0.6632557 (4000)\ttotal: 30.2s\tremaining: 12m 4s\n",
      "4500:\tlearn: 0.3835869\ttest: 0.6550853\tbest: 0.6550853 (4500)\ttotal: 33.6s\tremaining: 11m 53s\n",
      "5000:\tlearn: 0.3609617\ttest: 0.6472951\tbest: 0.6472935 (4999)\ttotal: 36.8s\tremaining: 11m 39s\n",
      "5500:\tlearn: 0.3401620\ttest: 0.6413938\tbest: 0.6413849 (5487)\ttotal: 40s\tremaining: 11m 27s\n",
      "6000:\tlearn: 0.3213307\ttest: 0.6352415\tbest: 0.6352415 (6000)\ttotal: 43.2s\tremaining: 11m 17s\n",
      "6500:\tlearn: 0.3044489\ttest: 0.6309312\tbest: 0.6309187 (6499)\ttotal: 46.8s\tremaining: 11m 13s\n",
      "7000:\tlearn: 0.2886508\ttest: 0.6269678\tbest: 0.6269633 (6999)\ttotal: 50.6s\tremaining: 11m 12s\n",
      "7500:\tlearn: 0.2739493\ttest: 0.6236264\tbest: 0.6236023 (7499)\ttotal: 54.3s\tremaining: 11m 9s\n",
      "8000:\tlearn: 0.2608520\ttest: 0.6214433\tbest: 0.6213899 (7989)\ttotal: 58.1s\tremaining: 11m 8s\n",
      "8500:\tlearn: 0.2488995\ttest: 0.6200573\tbest: 0.6200355 (8494)\ttotal: 1m 1s\tremaining: 11m 6s\n",
      "9000:\tlearn: 0.2372231\ttest: 0.6180838\tbest: 0.6180725 (8999)\ttotal: 1m 5s\tremaining: 11m 4s\n",
      "9500:\tlearn: 0.2263046\ttest: 0.6158759\tbest: 0.6158267 (9496)\ttotal: 1m 9s\tremaining: 11m 1s\n",
      "10000:\tlearn: 0.2164396\ttest: 0.6152862\tbest: 0.6149888 (9926)\ttotal: 1m 13s\tremaining: 10m 59s\n",
      "10500:\tlearn: 0.2072750\ttest: 0.6138779\tbest: 0.6138595 (10498)\ttotal: 1m 17s\tremaining: 10m 56s\n",
      "11000:\tlearn: 0.1983890\ttest: 0.6127674\tbest: 0.6127622 (10994)\ttotal: 1m 20s\tremaining: 10m 51s\n",
      "11500:\tlearn: 0.1902422\ttest: 0.6122481\tbest: 0.6119633 (11399)\ttotal: 1m 23s\tremaining: 10m 43s\n",
      "12000:\tlearn: 0.1824719\ttest: 0.6117128\tbest: 0.6116900 (11997)\ttotal: 1m 26s\tremaining: 10m 37s\n",
      "12500:\tlearn: 0.1752155\ttest: 0.6113077\tbest: 0.6111374 (12460)\ttotal: 1m 30s\tremaining: 10m 30s\n",
      "13000:\tlearn: 0.1683042\ttest: 0.6114237\tbest: 0.6111374 (12460)\ttotal: 1m 33s\tremaining: 10m 24s\n",
      "13500:\tlearn: 0.1618003\ttest: 0.6110853\tbest: 0.6106887 (13328)\ttotal: 1m 37s\tremaining: 10m 21s\n",
      "14000:\tlearn: 0.1554998\ttest: 0.6105516\tbest: 0.6105442 (13999)\ttotal: 1m 40s\tremaining: 10m 18s\n",
      "14500:\tlearn: 0.1494988\ttest: 0.6098554\tbest: 0.6098554 (14500)\ttotal: 1m 44s\tremaining: 10m 16s\n",
      "15000:\tlearn: 0.1437861\ttest: 0.6103765\tbest: 0.6098554 (14500)\ttotal: 1m 48s\tremaining: 10m 13s\n",
      "15500:\tlearn: 0.1382049\ttest: 0.6097017\tbest: 0.6095604 (15395)\ttotal: 1m 51s\tremaining: 10m 10s\n",
      "16000:\tlearn: 0.1329261\ttest: 0.6102362\tbest: 0.6095604 (15395)\ttotal: 1m 55s\tremaining: 10m 7s\n",
      "bestTest = 0.6095604332\n",
      "bestIteration = 15395\n",
      "Shrink model to first 15396 iterations.\n",
      "Iteration : 5 - CV Score : 0.5218195547138289\n",
      "================================================================================\n",
      "ending  training  : train score 0.16584100498429838 - oof Score 0.5385495227617011\n",
      "[0.5373412114802832, 0.526752094243457, 0.6015596382946056, 0.505286493135423, 0.5218195547138289, 0.5385495227617011]\n",
      "Counter({8: 436, 4: 252, 7: 127, 5: 118, 1: 56, 6: 46, 3: 31, 9: 8})\n",
      "(1074, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_id</th>\n",
       "      <th>crop_id_1</th>\n",
       "      <th>crop_id_2</th>\n",
       "      <th>crop_id_3</th>\n",
       "      <th>crop_id_4</th>\n",
       "      <th>crop_id_5</th>\n",
       "      <th>crop_id_6</th>\n",
       "      <th>crop_id_7</th>\n",
       "      <th>crop_id_8</th>\n",
       "      <th>crop_id_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002698</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.002243</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.050381</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.838813</td>\n",
       "      <td>0.102654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.003971</td>\n",
       "      <td>0.026647</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>0.896669</td>\n",
       "      <td>0.064566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.005153</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.004582</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>0.001879</td>\n",
       "      <td>0.941496</td>\n",
       "      <td>0.038996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.972461</td>\n",
       "      <td>0.021053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.917805</td>\n",
       "      <td>0.077824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28</td>\n",
       "      <td>0.003765</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.759528</td>\n",
       "      <td>0.007766</td>\n",
       "      <td>0.005611</td>\n",
       "      <td>0.027965</td>\n",
       "      <td>0.150015</td>\n",
       "      <td>0.037657</td>\n",
       "      <td>0.007548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.827519</td>\n",
       "      <td>0.166516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.007129</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.014132</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.635647</td>\n",
       "      <td>0.337312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>0.072190</td>\n",
       "      <td>0.014890</td>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.081704</td>\n",
       "      <td>0.805782</td>\n",
       "      <td>0.015325</td>\n",
       "      <td>0.004369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.003220</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.004197</td>\n",
       "      <td>0.055654</td>\n",
       "      <td>0.931888</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>45</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.123925</td>\n",
       "      <td>0.009182</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.028539</td>\n",
       "      <td>0.806949</td>\n",
       "      <td>0.013641</td>\n",
       "      <td>0.000984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>48</td>\n",
       "      <td>0.013226</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.107086</td>\n",
       "      <td>0.168635</td>\n",
       "      <td>0.010193</td>\n",
       "      <td>0.044226</td>\n",
       "      <td>0.456018</td>\n",
       "      <td>0.182040</td>\n",
       "      <td>0.017864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>53</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.855812</td>\n",
       "      <td>0.019953</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.111656</td>\n",
       "      <td>0.004650</td>\n",
       "      <td>0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>54</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.663214</td>\n",
       "      <td>0.016826</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>0.059067</td>\n",
       "      <td>0.192402</td>\n",
       "      <td>0.060080</td>\n",
       "      <td>0.003908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>57</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.009673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>61</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.797630</td>\n",
       "      <td>0.004639</td>\n",
       "      <td>0.005238</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>0.141369</td>\n",
       "      <td>0.043168</td>\n",
       "      <td>0.000292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>63</td>\n",
       "      <td>0.044941</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.038478</td>\n",
       "      <td>0.274502</td>\n",
       "      <td>0.069490</td>\n",
       "      <td>0.017757</td>\n",
       "      <td>0.483032</td>\n",
       "      <td>0.064966</td>\n",
       "      <td>0.004837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>69</td>\n",
       "      <td>0.008170</td>\n",
       "      <td>0.014675</td>\n",
       "      <td>0.049248</td>\n",
       "      <td>0.084434</td>\n",
       "      <td>0.006858</td>\n",
       "      <td>0.156910</td>\n",
       "      <td>0.192358</td>\n",
       "      <td>0.457836</td>\n",
       "      <td>0.029511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>70</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.985070</td>\n",
       "      <td>0.009351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>72</td>\n",
       "      <td>0.004613</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.002621</td>\n",
       "      <td>0.004018</td>\n",
       "      <td>0.021911</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.011536</td>\n",
       "      <td>0.915228</td>\n",
       "      <td>0.029996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>79</td>\n",
       "      <td>0.045638</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.193281</td>\n",
       "      <td>0.036810</td>\n",
       "      <td>0.435316</td>\n",
       "      <td>0.075794</td>\n",
       "      <td>0.157453</td>\n",
       "      <td>0.044996</td>\n",
       "      <td>0.005470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>80</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.006729</td>\n",
       "      <td>0.026878</td>\n",
       "      <td>0.019090</td>\n",
       "      <td>0.708075</td>\n",
       "      <td>0.092155</td>\n",
       "      <td>0.037787</td>\n",
       "      <td>0.078928</td>\n",
       "      <td>0.007526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>86</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.023693</td>\n",
       "      <td>0.891525</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.008835</td>\n",
       "      <td>0.039150</td>\n",
       "      <td>0.029416</td>\n",
       "      <td>0.003220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>87</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.036298</td>\n",
       "      <td>0.785733</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.010592</td>\n",
       "      <td>0.137907</td>\n",
       "      <td>0.021331</td>\n",
       "      <td>0.002868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>88</td>\n",
       "      <td>0.017960</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.068935</td>\n",
       "      <td>0.076886</td>\n",
       "      <td>0.623422</td>\n",
       "      <td>0.028438</td>\n",
       "      <td>0.175944</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>0.000798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>89</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>0.985816</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>0.002519</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.000271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>90</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.988401</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.000673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>92</td>\n",
       "      <td>0.022933</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.039740</td>\n",
       "      <td>0.512509</td>\n",
       "      <td>0.045818</td>\n",
       "      <td>0.018006</td>\n",
       "      <td>0.279214</td>\n",
       "      <td>0.069545</td>\n",
       "      <td>0.011366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>99</td>\n",
       "      <td>0.492778</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.007103</td>\n",
       "      <td>0.003059</td>\n",
       "      <td>0.011536</td>\n",
       "      <td>0.008369</td>\n",
       "      <td>0.011868</td>\n",
       "      <td>0.455519</td>\n",
       "      <td>0.009607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>100</td>\n",
       "      <td>0.939210</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.004758</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.007390</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.039093</td>\n",
       "      <td>0.002360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    field_id  crop_id_1  crop_id_2  crop_id_3  crop_id_4  crop_id_5  \\\n",
       "0          5   0.002698   0.000020   0.000890   0.002243   0.001302   \n",
       "1          6   0.002292   0.000035   0.001173   0.002939   0.003971   \n",
       "2         10   0.005153   0.000031   0.004582   0.003271   0.000768   \n",
       "3         18   0.000412   0.000030   0.001695   0.000700   0.000345   \n",
       "4         23   0.000564   0.000059   0.000750   0.000250   0.000401   \n",
       "5         28   0.003765   0.000145   0.759528   0.007766   0.005611   \n",
       "6         32   0.000447   0.000013   0.000277   0.000178   0.000447   \n",
       "7         35   0.001562   0.000063   0.007129   0.001214   0.000850   \n",
       "8         43   0.000818   0.002472   0.072190   0.014890   0.002451   \n",
       "9         44   0.000074   0.000550   0.003220   0.003311   0.004197   \n",
       "10        45   0.002459   0.000718   0.123925   0.009182   0.013601   \n",
       "11        48   0.013226   0.000711   0.107086   0.168635   0.010193   \n",
       "12        53   0.000619   0.000192   0.855812   0.019953   0.005291   \n",
       "13        54   0.000955   0.000959   0.663214   0.016826   0.002588   \n",
       "14        57   0.000722   0.000441   0.000525   0.000289   0.001088   \n",
       "15        61   0.005364   0.000550   0.797630   0.004639   0.005238   \n",
       "16        63   0.044941   0.001997   0.038478   0.274502   0.069490   \n",
       "17        69   0.008170   0.014675   0.049248   0.084434   0.006858   \n",
       "18        70   0.000917   0.000080   0.000523   0.000560   0.001621   \n",
       "19        72   0.004613   0.000565   0.002621   0.004018   0.021911   \n",
       "20        79   0.045638   0.005243   0.193281   0.036810   0.435316   \n",
       "21        80   0.022832   0.006729   0.026878   0.019090   0.708075   \n",
       "22        86   0.002038   0.000260   0.023693   0.891525   0.001864   \n",
       "23        87   0.003902   0.000305   0.036298   0.785733   0.001063   \n",
       "24        88   0.017960   0.000600   0.068935   0.076886   0.623422   \n",
       "25        89   0.001242   0.000037   0.002642   0.985816   0.000756   \n",
       "26        90   0.001235   0.000035   0.001830   0.988401   0.000725   \n",
       "27        92   0.022933   0.000870   0.039740   0.512509   0.045818   \n",
       "28        99   0.492778   0.000162   0.007103   0.003059   0.011536   \n",
       "29       100   0.939210   0.000033   0.004758   0.001625   0.007390   \n",
       "\n",
       "    crop_id_6  crop_id_7  crop_id_8  crop_id_9  \n",
       "0    0.050381   0.000999   0.838813   0.102654  \n",
       "1    0.026647   0.001708   0.896669   0.064566  \n",
       "2    0.003824   0.001879   0.941496   0.038996  \n",
       "3    0.002053   0.001253   0.972461   0.021053  \n",
       "4    0.001923   0.000424   0.917805   0.077824  \n",
       "5    0.027965   0.150015   0.037657   0.007548  \n",
       "6    0.004162   0.000441   0.827519   0.166516  \n",
       "7    0.014132   0.002092   0.635647   0.337312  \n",
       "8    0.081704   0.805782   0.015325   0.004369  \n",
       "9    0.055654   0.931888   0.000645   0.000461  \n",
       "10   0.028539   0.806949   0.013641   0.000984  \n",
       "11   0.044226   0.456018   0.182040   0.017864  \n",
       "12   0.001501   0.111656   0.004650   0.000326  \n",
       "13   0.059067   0.192402   0.060080   0.003908  \n",
       "14   0.001274   0.000805   0.985185   0.009673  \n",
       "15   0.001750   0.141369   0.043168   0.000292  \n",
       "16   0.017757   0.483032   0.064966   0.004837  \n",
       "17   0.156910   0.192358   0.457836   0.029511  \n",
       "18   0.000986   0.000892   0.985070   0.009351  \n",
       "19   0.009511   0.011536   0.915228   0.029996  \n",
       "20   0.075794   0.157453   0.044996   0.005470  \n",
       "21   0.092155   0.037787   0.078928   0.007526  \n",
       "22   0.008835   0.039150   0.029416   0.003220  \n",
       "23   0.010592   0.137907   0.021331   0.002868  \n",
       "24   0.028438   0.175944   0.007018   0.000798  \n",
       "25   0.002735   0.002519   0.003982   0.000271  \n",
       "26   0.003066   0.001844   0.002190   0.000673  \n",
       "27   0.018006   0.279214   0.069545   0.011366  \n",
       "28   0.008369   0.011868   0.455519   0.009607  \n",
       "29   0.002900   0.002632   0.039093   0.002360  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat_params = {\n",
    "            'learning_rate':0.01,\n",
    "            'max_depth':2,\n",
    "#             'eval_metric': 'Logloss',\n",
    "            'objective': 'MultiClass',\n",
    "            'od_type': 'Iter',\n",
    "            'l2_leaf_reg': 2,\n",
    "            'allow_writing_files': False,\n",
    "            'class_weights': class_weights\n",
    "        }\n",
    "y_test, cv_scores =kfold_catboost(X_train,X_test,y_train,\n",
    "                        params=cat_params,\n",
    "                        num_class=9,\n",
    "                        debug = True,\n",
    "                        num_folds = 5\n",
    "                        ,stratified = True\n",
    "                                 )\n",
    "print(cv_scores)\n",
    "prob = pd.DataFrame(y_test)\n",
    "\n",
    "predicted_classes = y_test.argmax(1)+1\n",
    "counter=collections.Counter(predicted_classes)\n",
    "print(counter)\n",
    "sns.countplot(predicted_classes, palette='Set3')\n",
    "\n",
    "# prob = pd.DataFrame(get_prediction_from_np_array(test_predict))\n",
    "prob.columns = ['crop_id_1', 'crop_id_2', 'crop_id_3', 'crop_id_4', 'crop_id_5', 'crop_id_6', 'crop_id_7','crop_id_8', 'crop_id_9']\n",
    "print(prob.shape)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['field_id'] = sample_sub['field_id']\n",
    "submission = submission.join(prob)\n",
    "submission.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('catboost.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
